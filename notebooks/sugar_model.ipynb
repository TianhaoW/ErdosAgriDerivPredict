{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sugar Prices\n",
    "\n",
    "The sugar prices data is taken from [Agricultural Futures Prices (Kaggle, originally Yahoo Finance)](https://www.kaggle.com/datasets/guillemservera/agricultural-futures)\n",
    "\n",
    "Historically, sugar production was important in the growth of slavery in Louisiana and in the U.S. annexation of Hawaii.\n",
    "\n",
    "Sugarcane \n",
    "* Florida: The largest sugarcane-producing region in the US, with most production in Palm Beach County\n",
    "* Louisiana: A major producer of sugarcane\n",
    "* Texas\n",
    "* Hawaii\n",
    "\n",
    "Sugar beets \n",
    "* California\n",
    "* Colorado\n",
    "* Idaho\n",
    "* Michigan\n",
    "* Minnesot\n",
    "* Montana\n",
    "* Nebraska\n",
    "* North Dakota\n",
    "* Oregon\n",
    "* Washington\n",
    "* Wyoming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from seaborn import set_style\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing \n",
    "\n",
    "We first need to get the data into a nice format. The production data is only annual, the weather data is daily, while the finance data is only on business days. We also don't have 2025 production data. My weather data goes up to March 19, 2025 (could get the most updated version though)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fin = pd.read_csv('../sugar_data/finance_data.csv')\n",
    "prod = pd.read_csv('../sugar_data/production_data.csv')\n",
    "weather = pd.read_csv('../sugar_data/weather_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    year    production\n",
      "0   2005  2.251679e+07\n",
      "1   2006  1.655668e+07\n",
      "2   2007  1.716306e+07\n",
      "3   2008  4.837029e+07\n",
      "4   2009  2.511015e+06\n",
      "5   2010  5.315556e+07\n",
      "6   2011  1.483909e+06\n",
      "7   2012  5.988276e+08\n",
      "8   2013  6.242756e+07\n",
      "9   2014  1.323900e+08\n",
      "10  2015  3.622623e+07\n",
      "11  2016  9.081382e+07\n",
      "12  2017  2.238663e+10\n",
      "13  2018  1.193610e+10\n",
      "14  2019  1.023751e+10\n",
      "15  2020  8.103290e+08\n",
      "16  2021  5.254510e+08\n",
      "17  2022  1.995750e+09\n",
      "18  2023  4.208000e+07\n",
      "19  2024  2.446300e+07\n"
     ]
    }
   ],
   "source": [
    "# Filter the DataFrame for the desired years.\n",
    "prod_filtered = prod[(prod['year'] >= 2005) & (prod['year'] <= 2024)]\n",
    "\n",
    "# Group by 'year' and sum the 'Value' column.\n",
    "prod_grouped = prod_filtered.groupby('year', as_index=False)['Value'].sum()\n",
    "\n",
    "# Rename the column to something more descriptive (e.g., 'production').\n",
    "prod_grouped.rename(columns={'Value': 'production'}, inplace=True)\n",
    "\n",
    "# Display the cleaned-up production data.\n",
    "print(prod_grouped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather.to_csv('../sugar_data/weather_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fin.to_csv('../sugar_data/finance_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the period of interest.\n",
    "start_date = '2005-01-01'\n",
    "end_date = '2025-03-19'\n",
    "\n",
    "# 1. Create a business day date range.\n",
    "business_dates = pd.date_range(start=start_date, end=end_date, freq='B')\n",
    "\n",
    "# 2. Impute annual production data onto the business days.\n",
    "# Assume prod_grouped is a DataFrame with columns ['year', 'production'].\n",
    "prod_expanded = pd.DataFrame(index=business_dates)\n",
    "prod_expanded['year'] = prod_expanded.index.year\n",
    "\n",
    "# Create a production series by setting the index to 'year'\n",
    "production_series = prod_grouped.set_index('year')['production']\n",
    "# Map each business day to the production value for that year.\n",
    "prod_expanded['production'] = prod_expanded['year'].map(production_series)\n",
    "# Optionally, drop the auxiliary 'year' column.\n",
    "prod_expanded.drop(columns=['year'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>production</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2005-01-03</th>\n",
       "      <td>22516789.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005-01-04</th>\n",
       "      <td>22516789.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005-01-05</th>\n",
       "      <td>22516789.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005-01-06</th>\n",
       "      <td>22516789.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005-01-07</th>\n",
       "      <td>22516789.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-03-13</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-03-14</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-03-17</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-03-18</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-03-19</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5273 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            production\n",
       "2005-01-03  22516789.0\n",
       "2005-01-04  22516789.0\n",
       "2005-01-05  22516789.0\n",
       "2005-01-06  22516789.0\n",
       "2005-01-07  22516789.0\n",
       "...                ...\n",
       "2025-03-13         NaN\n",
       "2025-03-14         NaN\n",
       "2025-03-17         NaN\n",
       "2025-03-18         NaN\n",
       "2025-03-19         NaN\n",
       "\n",
       "[5273 rows x 1 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prod_expanded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FL_max_temp_mean</th>\n",
       "      <th>FL_max_temp_var</th>\n",
       "      <th>FL_min_temp_mean</th>\n",
       "      <th>FL_min_temp_var</th>\n",
       "      <th>FL_avg_temp_mean</th>\n",
       "      <th>FL_avg_temp_var</th>\n",
       "      <th>FL_precip_mean</th>\n",
       "      <th>FL_precip_var</th>\n",
       "      <th>FL_snow_mean</th>\n",
       "      <th>FL_snow_var</th>\n",
       "      <th>LA_max_temp_mean</th>\n",
       "      <th>LA_max_temp_var</th>\n",
       "      <th>LA_min_temp_mean</th>\n",
       "      <th>LA_min_temp_var</th>\n",
       "      <th>LA_avg_temp_mean</th>\n",
       "      <th>LA_avg_temp_var</th>\n",
       "      <th>LA_precip_mean</th>\n",
       "      <th>LA_precip_var</th>\n",
       "      <th>LA_snow_mean</th>\n",
       "      <th>LA_snow_var</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2005-01-03</th>\n",
       "      <td>74.530074</td>\n",
       "      <td>7.791705</td>\n",
       "      <td>60.912226</td>\n",
       "      <td>2.057273</td>\n",
       "      <td>67.721526</td>\n",
       "      <td>2.682134</td>\n",
       "      <td>2.775408e-07</td>\n",
       "      <td>4.995726e-08</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>74.530074</td>\n",
       "      <td>7.791705</td>\n",
       "      <td>60.912226</td>\n",
       "      <td>2.057273</td>\n",
       "      <td>67.721526</td>\n",
       "      <td>2.682134</td>\n",
       "      <td>2.775408e-07</td>\n",
       "      <td>4.995726e-08</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005-01-04</th>\n",
       "      <td>76.341925</td>\n",
       "      <td>2.810333</td>\n",
       "      <td>58.742993</td>\n",
       "      <td>5.804291</td>\n",
       "      <td>67.542544</td>\n",
       "      <td>2.733289</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>76.341925</td>\n",
       "      <td>2.810333</td>\n",
       "      <td>58.742993</td>\n",
       "      <td>5.804291</td>\n",
       "      <td>67.542544</td>\n",
       "      <td>2.733289</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005-01-05</th>\n",
       "      <td>76.184676</td>\n",
       "      <td>2.990730</td>\n",
       "      <td>61.412292</td>\n",
       "      <td>3.618560</td>\n",
       "      <td>68.798794</td>\n",
       "      <td>2.513664</td>\n",
       "      <td>1.343158e-02</td>\n",
       "      <td>1.163041e-03</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>76.184676</td>\n",
       "      <td>2.990730</td>\n",
       "      <td>61.412292</td>\n",
       "      <td>3.618560</td>\n",
       "      <td>68.798794</td>\n",
       "      <td>2.513664</td>\n",
       "      <td>1.343158e-02</td>\n",
       "      <td>1.163041e-03</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005-01-06</th>\n",
       "      <td>74.942345</td>\n",
       "      <td>13.844986</td>\n",
       "      <td>54.127192</td>\n",
       "      <td>19.193575</td>\n",
       "      <td>64.534485</td>\n",
       "      <td>10.379555</td>\n",
       "      <td>1.479083e-01</td>\n",
       "      <td>1.193081e-02</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>74.942345</td>\n",
       "      <td>13.844986</td>\n",
       "      <td>54.127192</td>\n",
       "      <td>19.193575</td>\n",
       "      <td>64.534485</td>\n",
       "      <td>10.379555</td>\n",
       "      <td>1.479083e-01</td>\n",
       "      <td>1.193081e-02</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005-01-07</th>\n",
       "      <td>68.500560</td>\n",
       "      <td>44.847716</td>\n",
       "      <td>52.731653</td>\n",
       "      <td>14.136773</td>\n",
       "      <td>60.616413</td>\n",
       "      <td>16.100480</td>\n",
       "      <td>2.894701e-01</td>\n",
       "      <td>1.452934e-01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>68.500560</td>\n",
       "      <td>44.847716</td>\n",
       "      <td>52.731653</td>\n",
       "      <td>14.136773</td>\n",
       "      <td>60.616413</td>\n",
       "      <td>16.100480</td>\n",
       "      <td>2.894701e-01</td>\n",
       "      <td>1.452934e-01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-03-13</th>\n",
       "      <td>81.096189</td>\n",
       "      <td>2.651938</td>\n",
       "      <td>57.859202</td>\n",
       "      <td>9.009650</td>\n",
       "      <td>69.477348</td>\n",
       "      <td>4.458986</td>\n",
       "      <td>1.824144e-04</td>\n",
       "      <td>2.265630e-05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>81.096189</td>\n",
       "      <td>2.651938</td>\n",
       "      <td>57.859202</td>\n",
       "      <td>9.009650</td>\n",
       "      <td>69.477348</td>\n",
       "      <td>4.458986</td>\n",
       "      <td>1.824144e-04</td>\n",
       "      <td>2.265630e-05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-03-14</th>\n",
       "      <td>80.304420</td>\n",
       "      <td>2.595409</td>\n",
       "      <td>64.408529</td>\n",
       "      <td>1.396591</td>\n",
       "      <td>72.355977</td>\n",
       "      <td>0.835358</td>\n",
       "      <td>6.121287e-07</td>\n",
       "      <td>6.121250e-08</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>80.304420</td>\n",
       "      <td>2.595409</td>\n",
       "      <td>64.408529</td>\n",
       "      <td>1.396591</td>\n",
       "      <td>72.355977</td>\n",
       "      <td>0.835358</td>\n",
       "      <td>6.121287e-07</td>\n",
       "      <td>6.121250e-08</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-03-17</th>\n",
       "      <td>71.831232</td>\n",
       "      <td>8.497148</td>\n",
       "      <td>40.613640</td>\n",
       "      <td>2.985646</td>\n",
       "      <td>56.220881</td>\n",
       "      <td>2.924497</td>\n",
       "      <td>1.108208e-05</td>\n",
       "      <td>1.108086e-06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>71.831232</td>\n",
       "      <td>8.497148</td>\n",
       "      <td>40.613640</td>\n",
       "      <td>2.985646</td>\n",
       "      <td>56.220881</td>\n",
       "      <td>2.924497</td>\n",
       "      <td>1.108208e-05</td>\n",
       "      <td>1.108086e-06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-03-18</th>\n",
       "      <td>73.685133</td>\n",
       "      <td>1.898568</td>\n",
       "      <td>44.804156</td>\n",
       "      <td>5.522434</td>\n",
       "      <td>59.243382</td>\n",
       "      <td>2.392422</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>73.685133</td>\n",
       "      <td>1.898568</td>\n",
       "      <td>44.804156</td>\n",
       "      <td>5.522434</td>\n",
       "      <td>59.243382</td>\n",
       "      <td>2.392422</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-03-19</th>\n",
       "      <td>77.194302</td>\n",
       "      <td>8.626283</td>\n",
       "      <td>58.082187</td>\n",
       "      <td>17.317133</td>\n",
       "      <td>67.638185</td>\n",
       "      <td>11.338403</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>77.194302</td>\n",
       "      <td>8.626283</td>\n",
       "      <td>58.082187</td>\n",
       "      <td>17.317133</td>\n",
       "      <td>67.638185</td>\n",
       "      <td>11.338403</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5260 rows Ã— 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            FL_max_temp_mean  FL_max_temp_var  FL_min_temp_mean  \\\n",
       "date                                                              \n",
       "2005-01-03         74.530074         7.791705         60.912226   \n",
       "2005-01-04         76.341925         2.810333         58.742993   \n",
       "2005-01-05         76.184676         2.990730         61.412292   \n",
       "2005-01-06         74.942345        13.844986         54.127192   \n",
       "2005-01-07         68.500560        44.847716         52.731653   \n",
       "...                      ...              ...               ...   \n",
       "2025-03-13         81.096189         2.651938         57.859202   \n",
       "2025-03-14         80.304420         2.595409         64.408529   \n",
       "2025-03-17         71.831232         8.497148         40.613640   \n",
       "2025-03-18         73.685133         1.898568         44.804156   \n",
       "2025-03-19         77.194302         8.626283         58.082187   \n",
       "\n",
       "            FL_min_temp_var  FL_avg_temp_mean  FL_avg_temp_var  \\\n",
       "date                                                             \n",
       "2005-01-03         2.057273         67.721526         2.682134   \n",
       "2005-01-04         5.804291         67.542544         2.733289   \n",
       "2005-01-05         3.618560         68.798794         2.513664   \n",
       "2005-01-06        19.193575         64.534485        10.379555   \n",
       "2005-01-07        14.136773         60.616413        16.100480   \n",
       "...                     ...               ...              ...   \n",
       "2025-03-13         9.009650         69.477348         4.458986   \n",
       "2025-03-14         1.396591         72.355977         0.835358   \n",
       "2025-03-17         2.985646         56.220881         2.924497   \n",
       "2025-03-18         5.522434         59.243382         2.392422   \n",
       "2025-03-19        17.317133         67.638185        11.338403   \n",
       "\n",
       "            FL_precip_mean  FL_precip_var  FL_snow_mean  FL_snow_var  \\\n",
       "date                                                                   \n",
       "2005-01-03    2.775408e-07   4.995726e-08           0.0          0.0   \n",
       "2005-01-04    0.000000e+00   0.000000e+00           0.0          0.0   \n",
       "2005-01-05    1.343158e-02   1.163041e-03           0.0          0.0   \n",
       "2005-01-06    1.479083e-01   1.193081e-02           0.0          0.0   \n",
       "2005-01-07    2.894701e-01   1.452934e-01           0.0          0.0   \n",
       "...                    ...            ...           ...          ...   \n",
       "2025-03-13    1.824144e-04   2.265630e-05           0.0          0.0   \n",
       "2025-03-14    6.121287e-07   6.121250e-08           0.0          0.0   \n",
       "2025-03-17    1.108208e-05   1.108086e-06           0.0          0.0   \n",
       "2025-03-18    0.000000e+00   0.000000e+00           0.0          0.0   \n",
       "2025-03-19    0.000000e+00   0.000000e+00           0.0          0.0   \n",
       "\n",
       "            LA_max_temp_mean  LA_max_temp_var  LA_min_temp_mean  \\\n",
       "date                                                              \n",
       "2005-01-03         74.530074         7.791705         60.912226   \n",
       "2005-01-04         76.341925         2.810333         58.742993   \n",
       "2005-01-05         76.184676         2.990730         61.412292   \n",
       "2005-01-06         74.942345        13.844986         54.127192   \n",
       "2005-01-07         68.500560        44.847716         52.731653   \n",
       "...                      ...              ...               ...   \n",
       "2025-03-13         81.096189         2.651938         57.859202   \n",
       "2025-03-14         80.304420         2.595409         64.408529   \n",
       "2025-03-17         71.831232         8.497148         40.613640   \n",
       "2025-03-18         73.685133         1.898568         44.804156   \n",
       "2025-03-19         77.194302         8.626283         58.082187   \n",
       "\n",
       "            LA_min_temp_var  LA_avg_temp_mean  LA_avg_temp_var  \\\n",
       "date                                                             \n",
       "2005-01-03         2.057273         67.721526         2.682134   \n",
       "2005-01-04         5.804291         67.542544         2.733289   \n",
       "2005-01-05         3.618560         68.798794         2.513664   \n",
       "2005-01-06        19.193575         64.534485        10.379555   \n",
       "2005-01-07        14.136773         60.616413        16.100480   \n",
       "...                     ...               ...              ...   \n",
       "2025-03-13         9.009650         69.477348         4.458986   \n",
       "2025-03-14         1.396591         72.355977         0.835358   \n",
       "2025-03-17         2.985646         56.220881         2.924497   \n",
       "2025-03-18         5.522434         59.243382         2.392422   \n",
       "2025-03-19        17.317133         67.638185        11.338403   \n",
       "\n",
       "            LA_precip_mean  LA_precip_var  LA_snow_mean  LA_snow_var  \n",
       "date                                                                  \n",
       "2005-01-03    2.775408e-07   4.995726e-08           0.0          0.0  \n",
       "2005-01-04    0.000000e+00   0.000000e+00           0.0          0.0  \n",
       "2005-01-05    1.343158e-02   1.163041e-03           0.0          0.0  \n",
       "2005-01-06    1.479083e-01   1.193081e-02           0.0          0.0  \n",
       "2005-01-07    2.894701e-01   1.452934e-01           0.0          0.0  \n",
       "...                    ...            ...           ...          ...  \n",
       "2025-03-13    1.824144e-04   2.265630e-05           0.0          0.0  \n",
       "2025-03-14    6.121287e-07   6.121250e-08           0.0          0.0  \n",
       "2025-03-17    1.108208e-05   1.108086e-06           0.0          0.0  \n",
       "2025-03-18    0.000000e+00   0.000000e+00           0.0          0.0  \n",
       "2025-03-19    0.000000e+00   0.000000e+00           0.0          0.0  \n",
       "\n",
       "[5260 rows x 20 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather_business = weather[weather.index.isin(business_dates)]\n",
    "weather_business"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_business = weather_business.reindex(business_dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Dividends</th>\n",
       "      <th>Stock Splits</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>Day</th>\n",
       "      <th>...</th>\n",
       "      <th>High-Close</th>\n",
       "      <th>Low-Close</th>\n",
       "      <th>TR</th>\n",
       "      <th>14D_ATR</th>\n",
       "      <th>Volume_Volatility_Ratio</th>\n",
       "      <th>14D_RSI</th>\n",
       "      <th>7D_MA</th>\n",
       "      <th>14D_MA</th>\n",
       "      <th>7D_EMA</th>\n",
       "      <th>14D_EMA</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2005-01-03</th>\n",
       "      <td>9.080000</td>\n",
       "      <td>9.200000</td>\n",
       "      <td>9.070000</td>\n",
       "      <td>9.170000</td>\n",
       "      <td>34423</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2005</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0.160000</td>\n",
       "      <td>0.030000</td>\n",
       "      <td>0.160000</td>\n",
       "      <td>0.143572</td>\n",
       "      <td>3.060048e+06</td>\n",
       "      <td>73.076905</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>8.835714</td>\n",
       "      <td>9.008788</td>\n",
       "      <td>8.914379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005-01-04</th>\n",
       "      <td>9.190000</td>\n",
       "      <td>9.230000</td>\n",
       "      <td>9.020000</td>\n",
       "      <td>9.030000</td>\n",
       "      <td>24746</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2005</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>0.059999</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.209999</td>\n",
       "      <td>0.148571</td>\n",
       "      <td>2.000109e+06</td>\n",
       "      <td>63.793069</td>\n",
       "      <td>9.024286</td>\n",
       "      <td>8.858571</td>\n",
       "      <td>9.014091</td>\n",
       "      <td>8.929795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005-01-05</th>\n",
       "      <td>8.920000</td>\n",
       "      <td>8.980000</td>\n",
       "      <td>8.860000</td>\n",
       "      <td>8.980000</td>\n",
       "      <td>14882</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2005</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.170000</td>\n",
       "      <td>0.170000</td>\n",
       "      <td>0.149286</td>\n",
       "      <td>1.271632e+06</td>\n",
       "      <td>67.889846</td>\n",
       "      <td>9.041429</td>\n",
       "      <td>8.886429</td>\n",
       "      <td>9.005568</td>\n",
       "      <td>8.936489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005-01-06</th>\n",
       "      <td>8.980000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>8.880000</td>\n",
       "      <td>8.990000</td>\n",
       "      <td>47304</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2005</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.099999</td>\n",
       "      <td>0.120000</td>\n",
       "      <td>0.147143</td>\n",
       "      <td>4.111412e+06</td>\n",
       "      <td>70.754652</td>\n",
       "      <td>9.041429</td>\n",
       "      <td>8.917857</td>\n",
       "      <td>9.001676</td>\n",
       "      <td>8.943624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005-01-07</th>\n",
       "      <td>8.990000</td>\n",
       "      <td>9.010000</td>\n",
       "      <td>8.690000</td>\n",
       "      <td>8.710000</td>\n",
       "      <td>58376</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2005</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.320001</td>\n",
       "      <td>0.162143</td>\n",
       "      <td>3.989646e+06</td>\n",
       "      <td>59.055128</td>\n",
       "      <td>8.997143</td>\n",
       "      <td>8.934286</td>\n",
       "      <td>8.928757</td>\n",
       "      <td>8.912474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-03-13</th>\n",
       "      <td>18.840000</td>\n",
       "      <td>19.270000</td>\n",
       "      <td>18.760000</td>\n",
       "      <td>19.250000</td>\n",
       "      <td>47012</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2025</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>...</td>\n",
       "      <td>0.410000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.510000</td>\n",
       "      <td>0.630715</td>\n",
       "      <td>1.710616e+06</td>\n",
       "      <td>31.607149</td>\n",
       "      <td>18.610000</td>\n",
       "      <td>19.212857</td>\n",
       "      <td>18.880568</td>\n",
       "      <td>19.092382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-03-14</th>\n",
       "      <td>19.170000</td>\n",
       "      <td>19.320000</td>\n",
       "      <td>18.750000</td>\n",
       "      <td>19.190001</td>\n",
       "      <td>43753</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2025</td>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "      <td>...</td>\n",
       "      <td>0.070000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.570000</td>\n",
       "      <td>0.641429</td>\n",
       "      <td>1.591733e+06</td>\n",
       "      <td>32.536770</td>\n",
       "      <td>18.751429</td>\n",
       "      <td>19.077143</td>\n",
       "      <td>18.957926</td>\n",
       "      <td>19.105398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-03-17</th>\n",
       "      <td>19.250000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>19.219999</td>\n",
       "      <td>19.969999</td>\n",
       "      <td>89334</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2025</td>\n",
       "      <td>3</td>\n",
       "      <td>17</td>\n",
       "      <td>...</td>\n",
       "      <td>0.809999</td>\n",
       "      <td>0.029999</td>\n",
       "      <td>0.809999</td>\n",
       "      <td>0.655714</td>\n",
       "      <td>3.020160e+06</td>\n",
       "      <td>37.264960</td>\n",
       "      <td>19.014286</td>\n",
       "      <td>18.970714</td>\n",
       "      <td>19.210945</td>\n",
       "      <td>19.220678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-03-18</th>\n",
       "      <td>19.900000</td>\n",
       "      <td>20.090000</td>\n",
       "      <td>19.629999</td>\n",
       "      <td>19.990000</td>\n",
       "      <td>72006</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2025</td>\n",
       "      <td>3</td>\n",
       "      <td>18</td>\n",
       "      <td>...</td>\n",
       "      <td>0.120001</td>\n",
       "      <td>0.340000</td>\n",
       "      <td>0.460001</td>\n",
       "      <td>0.622858</td>\n",
       "      <td>2.576454e+06</td>\n",
       "      <td>43.564362</td>\n",
       "      <td>19.254286</td>\n",
       "      <td>18.924286</td>\n",
       "      <td>19.405708</td>\n",
       "      <td>19.323254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-03-19</th>\n",
       "      <td>19.860001</td>\n",
       "      <td>20.049999</td>\n",
       "      <td>19.540001</td>\n",
       "      <td>19.690001</td>\n",
       "      <td>48894</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2025</td>\n",
       "      <td>3</td>\n",
       "      <td>19</td>\n",
       "      <td>...</td>\n",
       "      <td>0.059999</td>\n",
       "      <td>0.449999</td>\n",
       "      <td>0.509998</td>\n",
       "      <td>0.570715</td>\n",
       "      <td>1.942599e+06</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>19.374286</td>\n",
       "      <td>18.924286</td>\n",
       "      <td>19.476781</td>\n",
       "      <td>19.372154</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5080 rows Ã— 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Open       High        Low      Close  Volume  Dividends  \\\n",
       "Date                                                                        \n",
       "2005-01-03   9.080000   9.200000   9.070000   9.170000   34423        0.0   \n",
       "2005-01-04   9.190000   9.230000   9.020000   9.030000   24746        0.0   \n",
       "2005-01-05   8.920000   8.980000   8.860000   8.980000   14882        0.0   \n",
       "2005-01-06   8.980000   9.000000   8.880000   8.990000   47304        0.0   \n",
       "2005-01-07   8.990000   9.010000   8.690000   8.710000   58376        0.0   \n",
       "...               ...        ...        ...        ...     ...        ...   \n",
       "2025-03-13  18.840000  19.270000  18.760000  19.250000   47012        0.0   \n",
       "2025-03-14  19.170000  19.320000  18.750000  19.190001   43753        0.0   \n",
       "2025-03-17  19.250000  20.000000  19.219999  19.969999   89334        0.0   \n",
       "2025-03-18  19.900000  20.090000  19.629999  19.990000   72006        0.0   \n",
       "2025-03-19  19.860001  20.049999  19.540001  19.690001   48894        0.0   \n",
       "\n",
       "            Stock Splits  Year  Month  Day  ...  High-Close Low-Close  \\\n",
       "Date                                        ...                         \n",
       "2005-01-03           0.0  2005      1    3  ...    0.160000  0.030000   \n",
       "2005-01-04           0.0  2005      1    4  ...    0.059999  0.150000   \n",
       "2005-01-05           0.0  2005      1    5  ...    0.050000  0.170000   \n",
       "2005-01-06           0.0  2005      1    6  ...    0.020000  0.099999   \n",
       "2005-01-07           0.0  2005      1    7  ...    0.020000  0.300000   \n",
       "...                  ...   ...    ...  ...  ...         ...       ...   \n",
       "2025-03-13           0.0  2025      3   13  ...    0.410000  0.100000   \n",
       "2025-03-14           0.0  2025      3   14  ...    0.070000  0.500000   \n",
       "2025-03-17           0.0  2025      3   17  ...    0.809999  0.029999   \n",
       "2025-03-18           0.0  2025      3   18  ...    0.120001  0.340000   \n",
       "2025-03-19           0.0  2025      3   19  ...    0.059999  0.449999   \n",
       "\n",
       "                  TR   14D_ATR  Volume_Volatility_Ratio    14D_RSI      7D_MA  \\\n",
       "Date                                                                            \n",
       "2005-01-03  0.160000  0.143572             3.060048e+06  73.076905   9.000000   \n",
       "2005-01-04  0.209999  0.148571             2.000109e+06  63.793069   9.024286   \n",
       "2005-01-05  0.170000  0.149286             1.271632e+06  67.889846   9.041429   \n",
       "2005-01-06  0.120000  0.147143             4.111412e+06  70.754652   9.041429   \n",
       "2005-01-07  0.320001  0.162143             3.989646e+06  59.055128   8.997143   \n",
       "...              ...       ...                      ...        ...        ...   \n",
       "2025-03-13  0.510000  0.630715             1.710616e+06  31.607149  18.610000   \n",
       "2025-03-14  0.570000  0.641429             1.591733e+06  32.536770  18.751429   \n",
       "2025-03-17  0.809999  0.655714             3.020160e+06  37.264960  19.014286   \n",
       "2025-03-18  0.460001  0.622858             2.576454e+06  43.564362  19.254286   \n",
       "2025-03-19  0.509998  0.570715             1.942599e+06  50.000000  19.374286   \n",
       "\n",
       "               14D_MA     7D_EMA    14D_EMA  \n",
       "Date                                         \n",
       "2005-01-03   8.835714   9.008788   8.914379  \n",
       "2005-01-04   8.858571   9.014091   8.929795  \n",
       "2005-01-05   8.886429   9.005568   8.936489  \n",
       "2005-01-06   8.917857   9.001676   8.943624  \n",
       "2005-01-07   8.934286   8.928757   8.912474  \n",
       "...               ...        ...        ...  \n",
       "2025-03-13  19.212857  18.880568  19.092382  \n",
       "2025-03-14  19.077143  18.957926  19.105398  \n",
       "2025-03-17  18.970714  19.210945  19.220678  \n",
       "2025-03-18  18.924286  19.405708  19.323254  \n",
       "2025-03-19  18.924286  19.476781  19.372154  \n",
       "\n",
       "[5080 rows x 27 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_date = '2005-01-01'\n",
    "end_date = '2025-03-19'\n",
    "\n",
    "# Extract rows within the specified date range\n",
    "fin_small = fin.loc[start_date:end_date]\n",
    "\n",
    "fin_small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weather shape: (5080, 20)\n",
      "Production shape: (5080, 1)\n",
      "Finance shape: (5080, 27)\n"
     ]
    }
   ],
   "source": [
    "# Use the finance DataFrame's index as the reference\n",
    "fin_dates = fin_small.index\n",
    "\n",
    "# Reindex the weather and production DataFrames to have the same dates as fin_small.\n",
    "# This will leave NaN for any dates that are missing in the original data.\n",
    "weather_small = weather.reindex(fin_dates)\n",
    "prod_small = prod_expanded.reindex(fin_dates)\n",
    "\n",
    "print(\"Weather shape:\", weather_small.shape)\n",
    "print(\"Production shape:\", prod_small.shape)\n",
    "print(\"Finance shape:\", fin_small.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = pd.concat([fin_small,prod_small,weather_small],axis=1)\n",
    "combined_df = combined_df.ffill()\n",
    "combined_df['shift_Log_Return'] = combined_df['Log_Return'].shift(-1)\n",
    "# drop last row because of the shift\n",
    "combined_df = combined_df.drop(combined_df.index[-1])\n",
    "combined_df.to_csv(\"sugar_data/combined_sugar_data.csv\", index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models\n",
    "\n",
    "* SARIMA\n",
    "* Linear Regression, XGBoost\n",
    "* Ridge, Lasso, Random Forest, SVR\n",
    "* Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = combined_df.drop(columns=['expiry']) # we have the days to expiry already (DTE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 RMSE: 3769.5128695095636, R^2: -17138321376.42553\n",
      "Fold 2 RMSE: 6.165600094010441, R^2: -153019.93344975275\n",
      "Fold 3 RMSE: 0.6813745412024003, R^2: -1210.634682646486\n",
      "Fold 4 RMSE: 2.7292430374018455, R^2: -22656.6918999003\n",
      "Fold 5 RMSE: 0.5741887983873264, R^2: -1130.010502812517\n",
      "Average RMSE: 755.9326551961132\n",
      "Average R^2: -3427699878.739213\n"
     ]
    }
   ],
   "source": [
    "# Set up the target and features:\n",
    "y = combined_df['shift_Log_Return']\n",
    "X = combined_df.drop(columns=['shift_Log_Return'])\n",
    "features = list(X.columns)\n",
    "\n",
    "# Initialize TimeSeriesSplit and an empty list for errors.\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "errors = []\n",
    "\n",
    "# Use combined_df for splitting.\n",
    "for fold, (train_index, test_index) in enumerate(tscv.split(combined_df)):\n",
    "    # Get training and test sets for the fold.\n",
    "    train = combined_df.iloc[train_index]\n",
    "    test = combined_df.iloc[test_index]\n",
    "    \n",
    "    # Define the SARIMAX model.\n",
    "    if features:\n",
    "        model = sm.tsa.SARIMAX(train['shift_Log_Return'], exog=train[features],\n",
    "                               order=(30, 1, 1), seasonal_order=(0, 1, 0, 12))\n",
    "        \n",
    "        \n",
    "    else:\n",
    "        model = sm.tsa.SARIMAX(train['shift_Log_Return'],\n",
    "                               order=(30, 1, 1), seasonal_order=(0, 1, 0, 12))\n",
    "    \n",
    "    # Fit the model.\n",
    "    results = model.fit(disp=False)\n",
    "    \n",
    "    # Forecast for the test period using integer indexing.\n",
    "    start = len(train)\n",
    "    end = len(train) + len(test) - 1\n",
    "    if features:\n",
    "        pred = results.predict(start=start, end=end, exog=test[features])\n",
    "    else:\n",
    "        pred = results.predict(start=start, end=end)\n",
    "    \n",
    "    # Calculate the RMSE and R^2 for this fold.\n",
    "    rmse = np.sqrt(mean_squared_error(test['shift_Log_Return'], pred))\n",
    "    r2 = r2_score(test['shift_Log_Return'], pred)\n",
    "    errors.append((rmse, r2))\n",
    "    print(f\"Fold {fold+1} RMSE: {rmse}, R^2: {r2}\")\n",
    "\n",
    "# Convert errors to a NumPy array and calculate the average metrics.\n",
    "errors_array = np.array(errors)\n",
    "avg_rmse, avg_r2 = errors_array.mean(axis=0)\n",
    "print(\"Average RMSE:\", avg_rmse)\n",
    "print(\"Average R^2:\", avg_r2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1:\n",
      "  Linear Regression -> RMSE: 0.0305, R^2: -0.1186\n",
      "  XGBoost           -> RMSE: 0.0373, R^2: -0.6815\n",
      "Fold 2:\n",
      "  Linear Regression -> RMSE: 0.0204, R^2: -0.6807\n",
      "  XGBoost           -> RMSE: 0.0203, R^2: -0.6585\n",
      "Fold 3:\n",
      "  Linear Regression -> RMSE: 0.0207, R^2: -0.1225\n",
      "  XGBoost           -> RMSE: 0.0217, R^2: -0.2249\n",
      "Fold 4:\n",
      "  Linear Regression -> RMSE: 0.0181, R^2: 0.0036\n",
      "  XGBoost           -> RMSE: 0.0204, R^2: -0.2650\n",
      "Fold 5:\n",
      "  Linear Regression -> RMSE: 0.0173, R^2: -0.0303\n",
      "  XGBoost           -> RMSE: 0.0211, R^2: -0.5281\n",
      "\n",
      "Average Metrics:\n",
      "  Linear Regression -> Average RMSE: 0.0214, Average R^2: -0.1897\n",
      "  XGBoost           -> Average RMSE: 0.0242, Average R^2: -0.4716\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "import xgboost as xgb\n",
    "\n",
    "# Define target and features.\n",
    "y = combined_df['shift_Log_Return']\n",
    "X = combined_df.drop(columns=['shift_Log_Return'])\n",
    "# List of feature names.\n",
    "features = list(X.columns)\n",
    "\n",
    "# Set up time series cross-validation.\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "# Lists to store metrics for each model.\n",
    "lr_errors = []   # For Linear Regression: list of (rmse, r2)\n",
    "xgb_errors = []  # For XGBoost: list of (rmse, r2)\n",
    "\n",
    "for fold, (train_index, test_index) in enumerate(tscv.split(combined_df)):\n",
    "    # Create training and testing subsets.\n",
    "    train = combined_df.iloc[train_index]\n",
    "    test = combined_df.iloc[test_index]\n",
    "    \n",
    "    # Separate predictors and target.\n",
    "    X_train = train.drop(columns=['shift_Log_Return'])\n",
    "    y_train = train['shift_Log_Return']\n",
    "    X_test = test.drop(columns=['shift_Log_Return'])\n",
    "    y_test = test['shift_Log_Return']\n",
    "    \n",
    "    # ----- Linear Regression -----\n",
    "    lr_model = LinearRegression()\n",
    "    lr_model.fit(X_train, y_train)\n",
    "    lr_pred = lr_model.predict(X_test)\n",
    "    \n",
    "    lr_rmse = np.sqrt(mean_squared_error(y_test, lr_pred))\n",
    "    lr_r2 = r2_score(y_test, lr_pred)\n",
    "    lr_errors.append((lr_rmse, lr_r2))\n",
    "    \n",
    "    # ----- XGBoost Regressor -----\n",
    "    xgb_model = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=100)\n",
    "    xgb_model.fit(X_train, y_train)\n",
    "    xgb_pred = xgb_model.predict(X_test)\n",
    "    \n",
    "    xgb_rmse = np.sqrt(mean_squared_error(y_test, xgb_pred))\n",
    "    xgb_r2 = r2_score(y_test, xgb_pred)\n",
    "    xgb_errors.append((xgb_rmse, xgb_r2))\n",
    "    \n",
    "    print(f\"Fold {fold+1}:\")\n",
    "    print(f\"  Linear Regression -> RMSE: {lr_rmse:.4f}, R^2: {lr_r2:.4f}\")\n",
    "    print(f\"  XGBoost           -> RMSE: {xgb_rmse:.4f}, R^2: {xgb_r2:.4f}\")\n",
    "\n",
    "# Convert error lists to NumPy arrays to average metrics.\n",
    "lr_errors_array = np.array(lr_errors)\n",
    "avg_lr_rmse, avg_lr_r2 = lr_errors_array.mean(axis=0)\n",
    "\n",
    "xgb_errors_array = np.array(xgb_errors)\n",
    "avg_xgb_rmse, avg_xgb_r2 = xgb_errors_array.mean(axis=0)\n",
    "\n",
    "print(\"\\nAverage Metrics:\")\n",
    "print(f\"  Linear Regression -> Average RMSE: {avg_lr_rmse:.4f}, Average R^2: {avg_lr_r2:.4f}\")\n",
    "print(f\"  XGBoost           -> Average RMSE: {avg_xgb_rmse:.4f}, Average R^2: {avg_xgb_r2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1:\n",
      "  Ridge -> RMSE: 0.0300, RÂ²: -0.0843\n",
      "  Lasso -> RMSE: 0.0289, RÂ²: -0.0069\n",
      "  RF    -> RMSE: 0.0318, RÂ²: -0.2162\n",
      "  SVR   -> RMSE: 0.0357, RÂ²: -0.5360\n",
      "\n",
      "Fold 2:\n",
      "  Ridge -> RMSE: 0.0194, RÂ²: -0.5090\n",
      "  Lasso -> RMSE: 0.0159, RÂ²: -0.0197\n",
      "  RF    -> RMSE: 0.0159, RÂ²: -0.0148\n",
      "  SVR   -> RMSE: 0.0623, RÂ²: -14.6124\n",
      "\n",
      "Fold 3:\n",
      "  Ridge -> RMSE: 0.0198, RÂ²: -0.0255\n",
      "  Lasso -> RMSE: 0.0363, RÂ²: -2.4451\n",
      "  RF    -> RMSE: 0.0199, RÂ²: -0.0311\n",
      "  SVR   -> RMSE: 0.0237, RÂ²: -0.4719\n",
      "\n",
      "Fold 4:\n",
      "  Ridge -> RMSE: 0.0181, RÂ²: 0.0044\n",
      "  Lasso -> RMSE: 0.0182, RÂ²: -0.0032\n",
      "  RF    -> RMSE: 0.0179, RÂ²: 0.0261\n",
      "  SVR   -> RMSE: 0.0191, RÂ²: -0.1084\n",
      "\n",
      "Fold 5:\n",
      "  Ridge -> RMSE: 0.0174, RÂ²: -0.0348\n",
      "  Lasso -> RMSE: 0.0171, RÂ²: -0.0018\n",
      "  RF    -> RMSE: 0.0178, RÂ²: -0.0823\n",
      "  SVR   -> RMSE: 0.0176, RÂ²: -0.0673\n",
      "\n",
      "Average Metrics Across Folds:\n",
      "  Ridge -> Average RMSE: 0.0209, Average RÂ²: -0.1299\n",
      "  Lasso -> Average RMSE: 0.0233, Average RÂ²: -0.4953\n",
      "  RF    -> Average RMSE: 0.0206, Average RÂ²: -0.0637\n",
      "  SVR   -> Average RMSE: 0.0317, Average RÂ²: -3.1592\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "# Create target and features.\n",
    "y = combined_df['shift_Log_Return']\n",
    "X = combined_df.drop(columns=['shift_Log_Return'])\n",
    "features = list(X.columns)\n",
    "\n",
    "# Initialize TimeSeriesSplit.\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "# Lists to store metrics for each model.\n",
    "ridge_errors = []  # (RMSE, R^2) tuples for Ridge.\n",
    "lasso_errors = []  # (RMSE, R^2) tuples for Lasso.\n",
    "rf_errors = []     # (RMSE, R^2) tuples for Random Forest.\n",
    "svr_errors = []    # (RMSE, R^2) tuples for SVR.\n",
    "\n",
    "# Loop over each fold.\n",
    "for fold, (train_index, test_index) in enumerate(tscv.split(combined_df)):\n",
    "    train = combined_df.iloc[train_index]\n",
    "    test = combined_df.iloc[test_index]\n",
    "    \n",
    "    # Separate predictors and target.\n",
    "    X_train = train.drop(columns=['shift_Log_Return'])\n",
    "    y_train = train['shift_Log_Return']\n",
    "    X_test = test.drop(columns=['shift_Log_Return'])\n",
    "    y_test = test['shift_Log_Return']\n",
    "    \n",
    "    # ----- Ridge Regression -----\n",
    "    ridge_model = Ridge(alpha=1.0)\n",
    "    ridge_model.fit(X_train, y_train)\n",
    "    ridge_pred = ridge_model.predict(X_test)\n",
    "    ridge_rmse = np.sqrt(mean_squared_error(y_test, ridge_pred))\n",
    "    ridge_r2 = r2_score(y_test, ridge_pred)\n",
    "    ridge_errors.append((ridge_rmse, ridge_r2))\n",
    "    \n",
    "    # ----- Lasso Regression -----\n",
    "    lasso_model = Lasso(alpha=0.1)\n",
    "    lasso_model.fit(X_train, y_train)\n",
    "    lasso_pred = lasso_model.predict(X_test)\n",
    "    lasso_rmse = np.sqrt(mean_squared_error(y_test, lasso_pred))\n",
    "    lasso_r2 = r2_score(y_test, lasso_pred)\n",
    "    lasso_errors.append((lasso_rmse, lasso_r2))\n",
    "    \n",
    "    # ----- Random Forest Regressor -----\n",
    "    rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "    rf_model.fit(X_train, y_train)\n",
    "    rf_pred = rf_model.predict(X_test)\n",
    "    rf_rmse = np.sqrt(mean_squared_error(y_test, rf_pred))\n",
    "    rf_r2 = r2_score(y_test, rf_pred)\n",
    "    rf_errors.append((rf_rmse, rf_r2))\n",
    "    \n",
    "    # ----- Support Vector Regression -----\n",
    "    svr_model = SVR(kernel='rbf', C=1.0, epsilon=0.1)\n",
    "    svr_model.fit(X_train, y_train)\n",
    "    svr_pred = svr_model.predict(X_test)\n",
    "    svr_rmse = np.sqrt(mean_squared_error(y_test, svr_pred))\n",
    "    svr_r2 = r2_score(y_test, svr_pred)\n",
    "    svr_errors.append((svr_rmse, svr_r2))\n",
    "    \n",
    "    print(f\"Fold {fold+1}:\")\n",
    "    print(f\"  Ridge -> RMSE: {ridge_rmse:.4f}, R^2: {ridge_r2:.4f}\")\n",
    "    print(f\"  Lasso -> RMSE: {lasso_rmse:.4f}, R^2: {lasso_r2:.4f}\")\n",
    "    print(f\"  Random Forest    -> RMSE: {rf_rmse:.4f}, R^2: {rf_r2:.4f}\")\n",
    "    print(f\"  SVR   -> RMSE: {svr_rmse:.4f}, R^2: {svr_r2:.4f}\\n\")\n",
    "\n",
    "# Convert error lists to NumPy arrays and average metrics.\n",
    "ridge_errors_arr = np.array(ridge_errors)\n",
    "lasso_errors_arr = np.array(lasso_errors)\n",
    "rf_errors_arr = np.array(rf_errors)\n",
    "svr_errors_arr = np.array(svr_errors)\n",
    "\n",
    "avg_ridge_rmse, avg_ridge_r2 = ridge_errors_arr.mean(axis=0)\n",
    "avg_lasso_rmse, avg_lasso_r2 = lasso_errors_arr.mean(axis=0)\n",
    "avg_rf_rmse, avg_rf_r2 = rf_errors_arr.mean(axis=0)\n",
    "avg_svr_rmse, avg_svr_r2 = svr_errors_arr.mean(axis=0)\n",
    "\n",
    "print(\"Average Metrics Across Folds:\")\n",
    "print(f\"  Ridge -> Average RMSE: {avg_ridge_rmse:.4f}, Average R^2: {avg_ridge_r2:.4f}\")\n",
    "print(f\"  Lasso -> Average RMSE: {avg_lasso_rmse:.4f}, Average R^2: {avg_lasso_r2:.4f}\")\n",
    "print(f\"  Random Forest    -> Average RMSE: {avg_rf_rmse:.4f}, Average R^2: {avg_rf_r2:.4f}\")\n",
    "print(f\"  SVR   -> Average RMSE: {avg_svr_rmse:.4f}, Average R^2: {avg_svr_r2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler on the training data and transform it.\n",
    "scaled = scaler.fit_transform(combined_df)\n",
    "\n",
    "# Convert the NumPy arrays back into DataFrames.\n",
    "scaled_df = pd.DataFrame(scaled, index=combined_df.index, columns=combined_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 88\n",
      "Fold 1: NN RMSE: 8837.9265, NN RÂ²: -94210344665.0053\n",
      "Early stopping at epoch 37\n",
      "Fold 2: NN RMSE: 1548.4181, NN RÂ²: -9651095771.6688\n",
      "Early stopping at epoch 19\n",
      "Fold 3: NN RMSE: 7244811.5928, NN RÂ²: -136979028810712768.0000\n",
      "Early stopping at epoch 17\n",
      "Fold 4: NN RMSE: 6995.4700, NN RÂ²: -148855552456.0731\n",
      "Early stopping at epoch 14\n",
      "Fold 5: NN RMSE: 26488.1559, NN RÂ²: -2406914455840.9692\n",
      "\n",
      "Average Neural Network Metrics (PyTorch):\n",
      "  Average RMSE: 1457736.3127\n",
      "  Average RÂ²: -27396337688432300.0000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Set device for PyTorch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Prepare target and predictors as NumPy arrays.\n",
    "# combined_df should contain the target 'shift_Log_Return' and predictors.\n",
    "y = scaled_df['shift_Log_Return'].values\n",
    "X = scaled_df.drop(columns=['shift_Log_Return']).values\n",
    "\n",
    "# Define a simple feed-forward neural network model.\n",
    "class FeedForwardNN(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(FeedForwardNN, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# Training function with early stopping.\n",
    "def train_model(model, optimizer, criterion, X_train, y_train, X_val, y_val,\n",
    "                num_epochs=100, batch_size=32, patience=10):\n",
    "    model.train()\n",
    "    n_train = X_train.shape[0]\n",
    "    best_val_loss = np.inf\n",
    "    epochs_no_improve = 0\n",
    "    best_model_state = None\n",
    "\n",
    "    # Convert all training and validation data to tensors.\n",
    "    X_train_tensor = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "    y_train_tensor = torch.tensor(y_train, dtype=torch.float32).view(-1, 1).to(device)\n",
    "    X_val_tensor = torch.tensor(X_val, dtype=torch.float32).to(device)\n",
    "    y_val_tensor = torch.tensor(y_val, dtype=torch.float32).view(-1, 1).to(device)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        permutation = torch.randperm(n_train)\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        # Mini-batch training.\n",
    "        for i in range(0, n_train, batch_size):\n",
    "            optimizer.zero_grad()\n",
    "            indices = permutation[i:i+batch_size]\n",
    "            batch_x = X_train_tensor[indices]\n",
    "            batch_y = y_train_tensor[indices]\n",
    "            outputs = model(batch_x)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item() * batch_x.size(0)\n",
    "        \n",
    "        epoch_loss /= n_train\n",
    "\n",
    "        # Evaluate on validation data.\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_outputs = model(X_val_tensor)\n",
    "            val_loss = criterion(val_outputs, y_val_tensor).item()\n",
    "        \n",
    "        # Early stopping check.\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model_state = model.state_dict()\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "\n",
    "    # Restore best model state.\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Set up TimeSeriesSplit for cross-validation.\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "nn_errors = []  # To store (RMSE, R^2) for each fold.\n",
    "\n",
    "for fold, (train_index, test_index) in enumerate(tscv.split(combined_df)):\n",
    "    # Get fold data.\n",
    "    train_data = combined_df.iloc[train_index]\n",
    "    test_data = combined_df.iloc[test_index]\n",
    "    \n",
    "    X_train = train_data.drop(columns=['shift_Log_Return']).values\n",
    "    y_train = train_data['shift_Log_Return'].values\n",
    "    X_test = test_data.drop(columns=['shift_Log_Return']).values\n",
    "    y_test = test_data['shift_Log_Return'].values\n",
    "    \n",
    "    # Further split training data to have a validation set (e.g., 80/20 split).\n",
    "    split_idx = int(0.8 * X_train.shape[0])\n",
    "    X_train_part, X_val = X_train[:split_idx], X_train[split_idx:]\n",
    "    y_train_part, y_val = y_train[:split_idx], y_train[split_idx:]\n",
    "    \n",
    "    input_dim = X_train.shape[1]\n",
    "    model = FeedForwardNN(input_dim).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    # Train with early stopping.\n",
    "    model = train_model(model, optimizer, criterion,\n",
    "                        X_train_part, y_train_part, X_val, y_val,\n",
    "                        num_epochs=100, batch_size=32, patience=10)\n",
    "    \n",
    "    # Evaluate on the test set.\n",
    "    model.eval()\n",
    "    X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "    with torch.no_grad():\n",
    "        test_pred = model(X_test_tensor).cpu().numpy().flatten()\n",
    "    \n",
    "    nn_rmse = np.sqrt(mean_squared_error(y_test, test_pred))\n",
    "    nn_r2 = r2_score(y_test, test_pred)\n",
    "    nn_errors.append((nn_rmse, nn_r2))\n",
    "    \n",
    "    print(f\"Fold {fold+1}: NN RMSE: {nn_rmse:.4f}, NN R^2: {nn_r2:.4f}\")\n",
    "\n",
    "# Compute average metrics across folds.\n",
    "nn_errors_arr = np.array(nn_errors)\n",
    "avg_nn_rmse, avg_nn_r2 = nn_errors_arr.mean(axis=0)\n",
    "print(\"\\nAverage Neural Network Metrics (PyTorch):\")\n",
    "print(f\"  Average RMSE: {avg_nn_rmse:.4f}\")\n",
    "print(f\"  Average R^2: {avg_nn_r2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What happened??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "erdos_spring_2025",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
