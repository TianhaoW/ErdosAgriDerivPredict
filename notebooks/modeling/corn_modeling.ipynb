{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de2e96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "```xml\n",
    "<VSCode.Cell id=\"#VSC-edc8b810\" language=\"python\">\n",
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "sys.path.insert(0, '../src')\n",
    "\n",
    "\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from src.data.preprocess import extend_market_data\n",
    "\n",
    "# Define ticker symbols for corn and ethanol futures\n",
    "corn_ticker = \"ZC=F\"   # Corn Futures (CBOT)\n",
    "ethanol_ticker = \"ZS=F\"  # Ethanol Futures (NYMEX)\n",
    "corn = yf.Ticker(corn_ticker)\n",
    "ethanol = yf.Ticker(ethanol_ticker)\n",
    "corn_data = corn.history(start =\"2014-01-01\", end =\"2024-12-31\")\n",
    "ethanol_data = ethanol.history(start =\"2014-01-01\", end =\"2024-12-31\")\n",
    "\n",
    "corn_data = extend_market_data(corn_data)\n",
    "ethanol_data = extend_market_data(ethanol_data)\n",
    "\n",
    "features = list(corn_data.columns)\n",
    "features.remove('expiry')\n",
    "corn_data = corn_data[features]\n",
    "\n",
    "features = list(ethanol_data.columns)\n",
    "features.remove('expiry')\n",
    "ethanol_data = ethanol_data[features]\n",
    "\n",
    "if merged.index.name != 'Date':\n",
    "    merged = merged.set_index('Date')\n",
    "merged = merged.drop(columns = [\"next_day_increment\", \"next_3day_increment\", \"next_7day_increment\"])\n",
    "\n",
    "corn_data[\"Log_Return_Shift\"] = corn_data[\"Log_Return\"].shift(-1)\n",
    "\n",
    "from datetime import datetime\n",
    "# start_date2231 = datetime.fromisoformat(\"2014-01-02\")\n",
    "import shelve\n",
    "with shelve.open('feat_dict.db') as features:\n",
    "\n",
    "    # Convert your features dict to a DataFrame\n",
    "    features_df = pd.DataFrame.from_dict(features, orient='index')\n",
    "    features_df.index = pd.to_datetime(features_df.index)\n",
    "    def create_shifted_features(features_df, lags):\n",
    "        shifted_dfs = []\n",
    "        for lag in lags:\n",
    "            shifted = features_df.shift(lag)\n",
    "            shifted.columns = [f\"{col}_lag_{lag}\" for col in features_df.columns]\n",
    "            shifted_dfs.append(shifted)\n",
    "    \n",
    "        # Concatenate all lagged features side-by-side\n",
    "        return pd.concat(shifted_dfs, axis=1)\n",
    "\n",
    "    lags = [2**i for i in range(1)]\n",
    "    lagged_features = create_shifted_features(features_df, lags)\n",
    "    # print(lagged_features.columns)\n",
    "    features_df = features_df.join(lagged_features, how = 'inner')\n",
    "    # Combine with your existing df\n",
    "    corn_data_with_weather = corn_data.join(features_df, how='left')\n",
    "    corn_data_with_weather = corn_data_with_weather.dropna()\n",
    "merged = merged.drop(columns=corn_data_with_weather.columns.intersection(merged.columns))\n",
    "merged = merged.dropna()\n",
    "corn_data_with_weather = corn_data_with_weather.dropna()\n",
    "corn_data_with_weather = corn_data_with_weather.join(merged, how='inner')\n",
    "# from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "# poly_array = poly.fit_transform(corn_data_with_weather)\n",
    "# poly_feature_names = poly.get_feature_names_out(corn_data_with_weather.columns)\n",
    "# corn_data_with_weather = pd.DataFrame(poly_array, columns=poly_feature_names, index=corn_data_with_weather.index)\n",
    "\n",
    "corn_data_train = corn_data[:datetime.fromisoformat(\"2023-12-31\")]\n",
    "corn_data_test = corn_data[datetime.fromisoformat(\"2023-12-31\"):]\n",
    "corn_data_with_weather_train = corn_data_with_weather[:datetime.fromisoformat(\"2023-12-31\")]\n",
    "corn_data_with_weather_test = corn_data_with_weather[datetime.fromisoformat(\"2023-12-31\"):]\n",
    "\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell id=\"#VSC-d6dcc87c\" language=\"markdown\">\n",
    "Linear Regression with no weather data\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell id=\"#VSC-009b46fb\" language=\"python\">\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "corn_data2 = corn_data_train.dropna()\n",
    "\n",
    "lin = LinearRegression()\n",
    "features = list(corn_data.columns)\n",
    "features.remove('Log_Return_Shift')\n",
    "X = corn_data2[list(features)]\n",
    "y = corn_data2[\"Log_Return_Shift\"]\n",
    "tss = TimeSeriesSplit(n_splits = 10)\n",
    "r2 = 0\n",
    "for i, (train_index, test_index) in enumerate(tss.split(X)):\n",
    "    X_train = X.iloc[train_index]\n",
    "    X_test = X.iloc[test_index]\n",
    "    y_train = y.iloc[train_index]\n",
    "    y_test = y.iloc[test_index]\n",
    "    lin.fit(X_train,y_train)\n",
    "    predlin = lin.predict(X_test)\n",
    "    r2 = r2_score(y_test, predlin)\n",
    "    print(f\"R² Score Linear: {r2:.4f}\")\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell id=\"#VSC-5a9b500a\" language=\"markdown\">\n",
    "Linear regression with weather data\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell id=\"#VSC-bb1f78ea\" language=\"python\">\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "features = list(corn_data_with_weather.columns)\n",
    "features.remove('Log_Return_Shift')\n",
    "\n",
    "corn_data2 = corn_data_with_weather_train.dropna()\n",
    "\n",
    "lin_weather = LinearRegression()\n",
    "# features = [\"average_temperature_distribution_weighted_kurtosis\"]\n",
    "X = corn_data_with_weather_train[list(features)]\n",
    "y = corn_data_with_weather_train[\"Log_Return_Shift\"]\n",
    "tss = TimeSeriesSplit(n_splits = 10)\n",
    "r2 = 0\n",
    "for i, (train_index, test_index) in enumerate(tss.split(X)):\n",
    "    X_train = X.iloc[train_index]\n",
    "    X_test = X.iloc[test_index]\n",
    "    y_train = y.iloc[train_index]\n",
    "    y_test = y.iloc[test_index]\n",
    "    lin_weather.fit(X_train,y_train)\n",
    "    predlin = lin_weather.predict(X_test)\n",
    "    r2 = r2_score(y_test, predlin)\n",
    "    print(f\"R² Score Linear: {r2:.4f}\")\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell id=\"#VSC-6c583715\" language=\"python\">\n",
    "def obj(params):\n",
    "\n",
    "    if 'n_estimators' in params.keys():\n",
    "        params['n_estimators'] = int(params['n_estimators'])\n",
    "    if 'max_depth' in params.keys():\n",
    "        params['max_depth'] = int(params['max_depth'])\n",
    "    \n",
    "    \n",
    "    features = list(corn_data.columns)\n",
    "    features.remove('Log_Return_Shift')\n",
    "    \n",
    "    corn_data2 = corn_data_train.dropna()\n",
    "    X = corn_data2[features]\n",
    "    y = corn_data2[\"Log_Return_Shift\"]\n",
    "    tss = TimeSeriesSplit(n_splits = 5)\n",
    "    r2 = 0\n",
    "    for i, (train_index, test_index) in enumerate(tss.split(X)):\n",
    "        \n",
    "        X_train = X.iloc[train_index]\n",
    "        X_test = X.iloc[test_index]\n",
    "        y_train = y.iloc[train_index]\n",
    "        y_test = y.iloc[test_index]\n",
    "        \n",
    "        dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "        dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "        \n",
    "        xgb_model = xgb.XGBRegressor(**params)\n",
    "        xgb_model.fit(X_train, y_train)\n",
    "        y_pred = xgb_model.predict(X_test)\n",
    "        sc = r2_score(y_test, y_pred)\n",
    "        r2 += r2_score(y_test, y_pred)\n",
    "    return -r2\n",
    "\n",
    "def obj_with_weather(params):\n",
    "\n",
    "    if 'n_estimators' in params.keys():\n",
    "        params['n_estimators'] = int(params['n_estimators'])\n",
    "    if 'max_depth' in params.keys():\n",
    "        params['max_depth'] = int(params['max_depth'])\n",
    "    \n",
    "    \n",
    "    features = list(corn_data_with_weather_train.columns)\n",
    "    features.remove('Log_Return_Shift')\n",
    "\n",
    "    corn_data2 = corn_data_with_weather_train.dropna()\n",
    "    X = corn_data2[features]\n",
    "    y = corn_data2[\"Log_Return_Shift\"]\n",
    "    tss = TimeSeriesSplit(n_splits = 5)\n",
    "    r2 = 0\n",
    "    for i, (train_index, test_index) in enumerate(tss.split(X)):\n",
    "        \n",
    "        X_train = X.iloc[train_index]\n",
    "        X_test = X.iloc[test_index]\n",
    "        y_train = y.iloc[train_index]\n",
    "        y_test = y.iloc[test_index]\n",
    "        \n",
    "        dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "        dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "        \n",
    "        xgb_model = xgb.XGBRegressor(**params)\n",
    "        xgb_model.fit(X_train, y_train)\n",
    "        y_pred = xgb_model.predict(X_test)\n",
    "        sc = r2_score(y_test, y_pred)\n",
    "        r2 += r2_score(y_test, y_pred)\n",
    "    return -r2\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell id=\"#VSC-cb18665b\" language=\"python\">\n",
    "from hyperopt import fmin, tpe, hp\n",
    "# Define the hyperparameter space\n",
    "paramspace = {\n",
    "    'objective': 'reg:squarederror',  # Regression\n",
    "    'eval_metric': 'rmse',\n",
    "    'n_estimators': hp.quniform('n_estimators', 10,200,10),\n",
    "    'max_depth': hp.quniform('max_depth', 2, 8, 1),\n",
    "    'learning_rate': hp.loguniform('learning_rate', -5, -1),\n",
    "    'min_child_weight': hp.quniform('min_child_weight', 1, 100, 9),\n",
    "    'gamma': hp.quniform('gamma', 0, 5, 1),\n",
    "    'subsample': hp.uniform('subsample', .5, 1)\n",
    "}\n",
    "\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell id=\"#VSC-0751a2f4\" language=\"markdown\">\n",
    "xgboost with no weather data\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell id=\"#VSC-6dc1ed72\" language=\"python\">\n",
    "best_params_no_weather = fmin(obj, paramspace, algo=tpe.suggest, max_evals=1000)\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell id=\"#VSC-edf2dde1\" language=\"markdown\">\n",
    "xgboost with weather data\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell id=\"#VSC-533c3045\" language=\"python\">\n",
    "best_params_with_weather = fmin(obj_with_weather, paramspace, algo=tpe.suggest, max_evals=1000)\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell id=\"#VSC-86a6f47b\" language=\"markdown\">\n",
    "XGBoost with no weather data\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell id=\"#VSC-ae93e148\" language=\"python\">\n",
    "obj(best_params_no_weather)\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell id=\"#VSC-94d373dd\" language=\"markdown\">\n",
    "XGBoost with weather data\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell id=\"#VSC-5e224a67\" language=\"python\">\n",
    "obj_with_weather(best_params_with_weather)\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell id=\"#VSC-87fb632a\" language=\"markdown\">\n",
    "Neural network with no weather\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell id=\"#VSC-7d97f71a\" language=\"python\">\n",
    "combined_df = corn_data.dropna()\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler on the training data and transform it.\n",
    "scaled = scaler.fit_transform(combined_df)\n",
    "\n",
    "# Convert the NumPy arrays back into DataFrames.\n",
    "scaled_df = pd.DataFrame(scaled, index=combined_df.index, columns=combined_df.columns)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Set device for PyTorch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Prepare target and predictors as NumPy arrays.\n",
    "# combined_df should contain the target 'Log_Return_Shift' and predictors.\n",
    "y = scaled_df['Log_Return_Shift'].values\n",
    "X = scaled_df.drop(columns=['Log_Return_Shift']).values\n",
    "\n",
    "# Define a simple feed-forward neural network model.\n",
    "class FeedForwardNN(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(FeedForwardNN, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# Training function with early stopping.\n",
    "def train_model(model, optimizer, criterion, X_train, y_train, X_val, y_val,\n",
    "                num_epochs=100, batch_size=32, patience=10):\n",
    "    model.train()\n",
    "    n_train = X_train.shape[0]\n",
    "    best_val_loss = np.inf\n",
    "    epochs_no_improve = 0\n",
    "    best_model_state = None\n",
    "\n",
    "    # Convert all training and validation data to tensors.\n",
    "    X_train_tensor = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "    y_train_tensor = torch.tensor(y_train, dtype=torch.float32).view(-1, 1).to(device)\n",
    "    X_val_tensor = torch.tensor(X_val, dtype=torch.float32).to(device)\n",
    "    y_val_tensor = torch.tensor(y_val, dtype=torch.float32).view(-1, 1).to(device)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        permutation = torch.randperm(n_train)\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        # Mini-batch training.\n",
    "        for i in range(0, n_train, batch_size):\n",
    "            optimizer.zero_grad()\n",
    "            indices = permutation[i:i+batch_size]\n",
    "            batch_x = X_train_tensor[indices]\n",
    "            batch_y = y_train_tensor[indices]\n",
    "            outputs = model(batch_x)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item() * batch_x.size(0)\n",
    "        \n",
    "        epoch_loss /= n_train\n",
    "\n",
    "        # Evaluate on validation data.\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_outputs = model(X_val_tensor)\n",
    "            val_loss = criterion(val_outputs, y_val_tensor).item()\n",
    "        \n",
    "        # Early stopping check.\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model_state = model.state_dict()\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "\n",
    "    # Restore best model state.\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Set up TimeSeriesSplit for cross-validation.\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "nn_errors = []  # To store (RMSE, R^2) for each fold.\n",
    "\n",
    "for fold, (train_index, test_index) in enumerate(tscv.split(combined_df)):\n",
    "    # Get fold data.\n",
    "    train_data = combined_df.iloc[train_index]\n",
    "    test_data = combined_df.iloc[test_index]\n",
    "    \n",
    "    X_train = train_data.drop(columns=['Log_Return_Shift']).values\n",
    "    y_train = train_data['Log_Return_Shift'].values\n",
    "    X_test = test_data.drop(columns=['Log_Return_Shift']).values\n",
    "    y_test = test_data['Log_Return_Shift'].values\n",
    "    \n",
    "    # Further split training data to have a validation set (e.g., 80/20 split).\n",
    "    split_idx = int(0.8 * X_train.shape[0])\n",
    "    X_train_part, X_val = X_train[:split_idx], X_train[split_idx:]\n",
    "    y_train_part, y_val = y_train[:split_idx], y_train[split_idx:]\n",
    "    \n",
    "    input_dim = X_train.shape[1]\n",
    "    model = FeedForwardNN(input_dim).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    # Train with early stopping.\n",
    "    model = train_model(model, optimizer, criterion,\n",
    "                        X_train_part, y_train_part, X_val, y_val,\n",
    "                        num_epochs=100, batch_size=32, patience=10)\n",
    "    \n",
    "    # Evaluate on the test set.\n",
    "    model.eval()\n",
    "    X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "    with torch.no_grad():\n",
    "        test_pred = model(X_test_tensor).cpu().numpy().flatten()\n",
    "    \n",
    "    nn_rmse = np.sqrt(mean_squared_error(y_test, test_pred))\n",
    "    nn_r2 = r2_score(y_test, test_pred)\n",
    "    nn_errors.append((nn_rmse, nn_r2))\n",
    "    \n",
    "    print(f\"Fold {fold+1}: NN RMSE: {nn_rmse:.4f}, NN R^2: {nn_r2:.4f}\")\n",
    "\n",
    "# Compute average metrics across folds.\n",
    "nn_errors_arr = np.array(nn_errors)\n",
    "avg_nn_rmse, avg_nn_r2 = nn_errors_arr.mean(axis=0)\n",
    "print(\"\\nAverage Neural Network Metrics (PyTorch):\")\n",
    "print(f\"  Average RMSE: {avg_nn_rmse:.4f}\")\n",
    "print(f\"  Average R^2: {avg_nn_r2:.4f}\")\n",
    "\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell id=\"#VSC-167a753f\" language=\"markdown\">\n",
    "Neural network with weather\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell id=\"#VSC-9daab0c0\" language=\"python\">\n",
    "combined_df = corn_data_with_weather.dropna()\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler on the training data and transform it.\n",
    "scaled = scaler.fit_transform(combined_df)\n",
    "\n",
    "# Convert the NumPy arrays back into DataFrames.\n",
    "scaled_df = pd.DataFrame(scaled, index=combined_df.index, columns=combined_df.columns)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Set device for PyTorch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Prepare target and predictors as NumPy arrays.\n",
    "# combined_df should contain the target 'Log_Return_Shift' and predictors.\n",
    "y = scaled_df['Log_Return_Shift'].values\n",
    "X = scaled_df.drop(columns=['Log_Return_Shift']).values\n",
    "\n",
    "# Define a simple feed-forward neural network model.\n",
    "class FeedForwardNN(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(FeedForwardNN, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# Training function with early stopping.\n",
    "def train_model(model, optimizer, criterion, X_train, y_train, X_val, y_val,\n",
    "                num_epochs=100, batch_size=32, patience=10):\n",
    "    model.train()\n",
    "    n_train = X_train.shape[0]\n",
    "    best_val_loss = np.inf\n",
    "    epochs_no_improve = 0\n",
    "    best_model_state = None\n",
    "\n",
    "    # Convert all training and validation data to tensors.\n",
    "    X_train_tensor = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "    y_train_tensor = torch.tensor(y_train, dtype=torch.float32).view(-1, 1).to(device)\n",
    "    X_val_tensor = torch.tensor(X_val, dtype=torch.float32).to(device)\n",
    "    y_val_tensor = torch.tensor(y_val, dtype=torch.float32).view(-1, 1).to(device)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        permutation = torch.randperm(n_train)\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        # Mini-batch training.\n",
    "        for i in range(0, n_train, batch_size):\n",
    "            optimizer.zero_grad()\n",
    "            indices = permutation[i:i+batch_size]\n",
    "            batch_x = X_train_tensor[indices]\n",
    "            batch_y = y_train_tensor[indices]\n",
    "            outputs = model(batch_x)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item() * batch_x.size(0)\n",
    "        \n",
    "        epoch_loss /= n_train\n",
    "\n",
    "        # Evaluate on validation data.\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_outputs = model(X_val_tensor)\n",
    "            val_loss = criterion(val_outputs, y_val_tensor).item()\n",
    "        \n",
    "        # Early stopping check.\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model_state = model.state_dict()\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "\n",
    "    # Restore best model state.\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Set up TimeSeriesSplit for cross-validation.\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "nn_errors = []  # To store (RMSE, R^2) for each fold.\n",
    "\n",
    "for fold, (train_index, test_index) in enumerate(tscv.split(combined_df)):\n",
    "    # Get fold data.\n",
    "    train_data = combined_df.iloc[train_index]\n",
    "    test_data = combined_df.iloc[test_index]\n",
    "    \n",
    "    X_train = train_data.drop(columns=['Log_Return_Shift']).values\n",
    "    y_train = train_data['Log_Return_Shift'].values\n",
    "    X_test = test_data.drop(columns=['Log_Return_Shift']).values\n",
    "    y_test = test_data['Log_Return_Shift'].values\n",
    "    \n",
    "    # Further split training data to have a validation set (e.g., 80/20 split).\n",
    "    split_idx = int(0.8 * X_train.shape[0])\n",
    "    X_train_part, X_val = X_train[:split_idx], X_train[split_idx:]\n",
    "    y_train_part, y_val = y_train[:split_idx], y_train[split_idx:]\n",
    "    \n",
    "    input_dim = X_train.shape[1]\n",
    "    model = FeedForwardNN(input_dim).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    # Train with early stopping.\n",
    "    model = train_model(model, optimizer, criterion,\n",
    "                        X_train_part, y_train_part, X_val, y_val,\n",
    "                        num_epochs=100, batch_size=32, patience=10)\n",
    "    \n",
    "    # Evaluate on the test set.\n",
    "    model.eval()\n",
    "    X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "    with torch.no_grad():\n",
    "        test_pred = model(X_test_tensor).cpu().numpy().flatten()\n",
    "    \n",
    "    nn_rmse = np.sqrt(mean_squared_error(y_test, test_pred))\n",
    "    nn_r2 = r2_score(y_test, test_pred)\n",
    "    nn_errors.append((nn_rmse, nn_r2))\n",
    "    \n",
    "    print(f\"Fold {fold+1}: NN RMSE: {nn_rmse:.4f}, NN R^2: {nn_r2:.4f}\")\n",
    "\n",
    "# Compute average metrics across folds.\n",
    "nn_errors_arr = np.array(nn_errors)\n",
    "avg_nn_rmse, avg_nn_r2 = nn_errors_arr.mean(axis=0)\n",
    "print(\"\\nAverage Neural Network Metrics (PyTorch):\")\n",
    "print(f\"  Average RMSE: {avg_nn_rmse:.4f}\")\n",
    "print(f\"  Average R^2: {avg_nn_r2:.4f}\")\n",
    "\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell id=\"#VSC-a95b3e71\" language=\"markdown\">\n",
    "LSTM Models\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell id=\"#VSC-ed3256df\" language=\"python\">\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "device = \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell id=\"#VSC-37737af9\" language=\"markdown\">\n",
    "LSTM with no weather\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell id=\"#VSC-12873099\" language=\"python\">\n",
    "data = corn_data.dropna()\n",
    "\n",
    "def create_sequences(data, seq_length, features):\n",
    "    scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "    data_scaled = scaler.fit_transform(data)\n",
    "    \n",
    "    features = list(data.columns)\n",
    "    features.remove(\"Log_Return_Shift\")\n",
    "    \n",
    "    feature_frame = data[features]\n",
    "    target_series = data[\"Log_Return_Shift\"]\n",
    "    sequences = []\n",
    "    targets = []\n",
    "    for i in range(len(data) - seq_length):\n",
    "        sequences.append(feature_frame.iloc[i:i+seq_length])\n",
    "        targets.append(target_series.iloc[i+seq_length])\n",
    "    return np.array(sequences), np.array(targets)\n",
    "\n",
    "\n",
    "\n",
    "SEQ_LENGTH = 60\n",
    "X, y = create_sequences(data, SEQ_LENGTH, features)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train, y_train = torch.tensor(X[:-500], dtype=torch.float32).to(device), torch.tensor(y[:-500], dtype=torch.float32).to(device)\n",
    "X_test, y_test = torch.tensor(X[-500:], dtype=torch.float32).to(device), torch.tensor(y[-500:], dtype=torch.float32).to(device)\n",
    "\n",
    "# print(X_train.shape[2])\n",
    "# Reshape for LSTM (batch_size, seq_length, num_features)\n",
    "X_train = X_train.view(-1, SEQ_LENGTH, X_train.shape[2])\n",
    "X_test = X_test.view(-1, SEQ_LENGTH, X_train.shape[2])\n",
    "\n",
    "# Create DataLoader\n",
    "BATCH_SIZE = 32\n",
    "train_loader = torch.utils.data.DataLoader(TensorDataset(X_train, y_train), batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(TensorDataset(X_test, y_test), batch_size=BATCH_SIZE, shuffle=False)\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell id=\"#VSC-f4f5f184\" language=\"python\">\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=64, num_layers=2, output_dim=1):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        # print(lstm_out)\n",
    "        out = self.fc(lstm_out[:, -1, :])  # Take last output from LSTM\n",
    "        # print(out)\n",
    "        return out\n",
    "\n",
    "# Initialize Model\n",
    "model = LSTMModel(input_dim = X_train.shape[2]).to(device)\n",
    "\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell id=\"#VSC-864207dc\" language=\"python\">\n",
    "# Define Loss and Optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training Loop\n",
    "EPOCHS = 100\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for batch_x, batch_y in train_loader:\n",
    "        # if(torch.isnan(batch_x).any() or torch.isinf(batch_x).any() or torch.isnan(batch_y).any() or torch.isinf(batch_y).any()):\n",
    "        #     print(\"hi\")\n",
    "        # print(torch.isnan(batch_x).any(), torch.isinf(batch_x).any())\n",
    "        # print(torch.isnan(batch_y).any(), torch.isinf(batch_y).any())\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(batch_x)\n",
    "        # print(y_pred)\n",
    "        loss = criterion(y_pred.squeeze(), batch_y)\n",
    "        # print(y_pred.squeeze())\n",
    "        # print(batch_y)\n",
    "        # print(loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{EPOCHS}], Loss: {epoch_loss/len(train_loader):.6f}')\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell id=\"#VSC-d35e8678\" language=\"python\">\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred_test = model(X_test).squeeze().cpu().numpy()\n",
    "\n",
    "# Inverse transform predictions\n",
    "# y_pred_test_rescaled = scaler.inverse_transform(y_pred_test.reshape(-1, 1))\n",
    "# y_test_rescaled = scaler.inverse_transform(y_test.numpy().reshape(-1, 1))\n",
    "\n",
    "y_pred_test_rescaled = y_pred_test.reshape(-1, 1)\n",
    "y_test_rescaled = y_test.cpu().numpy().reshape(-1, 1)\n",
    "\n",
    "# Plot results\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(data.index[-500:], y_test_rescaled, label='Actual')\n",
    "plt.plot(data.index[-500:], y_pred_test_rescaled, label='Predicted')\n",
    "plt.legend()\n",
    "plt.title(\"LSTM Model Predictions on testing set\")\n",
    "plt.show()\n",
    "\n",
    "print(f\"r2 score {r2_score(y_test_rescaled, y_pred_test_rescaled):.4f}\")\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell id=\"#VSC-afad3d49\" language=\"markdown\">\n",
    "LSTM with weather\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell id=\"#VSC-e6bb5c2e\" language=\"python\">\n",
    "data = corn_data_with_weather.dropna()\n",
    "\n",
    "def create_sequences(data, seq_length, features):\n",
    "    scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "    data_scaled = scaler.fit_transform(data)\n",
    "    \n",
    "    features = list(data.columns)\n",
    "    features.remove(\"Log_Return_Shift\")\n",
    "    \n",
    "    feature_frame = data[features]\n",
    "    target_series = data[\"Log_Return_Shift\"]\n",
    "    sequences = []\n",
    "    targets = []\n",
    "    for i in range(len(data) - seq_length):\n",
    "        sequences.append(feature_frame.iloc[i:i+seq_length])\n",
    "        targets.append(target_series.iloc[i+seq_length])\n",
    "    return np.array(sequences), np.array(targets)\n",
    "\n",
    "\n",
    "\n",
    "SEQ_LENGTH = 60\n",
    "X, y = create_sequences(data, SEQ_LENGTH, features)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train, y_train = torch.tensor(X[:-500], dtype=torch.float32).to(device), torch.tensor(y[:-500], dtype=torch.float32).to(device)\n",
    "X_test, y_test = torch.tensor(X[-500:], dtype=torch.float32).to(device), torch.tensor(y[-500:], dtype=torch.float32).to(device)\n",
    "\n",
    "# print(X_train.shape[2])\n",
    "# Reshape for LSTM (batch_size, seq_length, num_features)\n",
    "X_train = X_train.view(-1, SEQ_LENGTH, X_train.shape[2])\n",
    "X_test = X_test.view(-1, SEQ_LENGTH, X_train.shape[2])\n",
    "\n",
    "# Create DataLoader\n",
    "BATCH_SIZE = 32\n",
    "train_loader = torch.utils.data.DataLoader(TensorDataset(X_train, y_train), batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(TensorDataset(X_test, y_test), batch_size=BATCH_SIZE, shuffle=False)\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell id=\"#VSC-4e67a22b\" language=\"python\">\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=64, num_layers=2, output_dim=1):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        # print(lstm_out)\n",
    "        out = self.fc(lstm_out[:, -1, :])  # Take last output from LSTM\n",
    "        # print(out)\n",
    "        return out\n",
    "\n",
    "# Initialize Model\n",
    "model = LSTMModel(input_dim = X_train.shape[2]).to(device)\n",
    "\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell id=\"#VSC-fb842d66\" language=\"python\">\n",
    "# Define Loss and Optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training Loop\n",
    "EPOCHS = 100\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for batch_x, batch_y in train_loader:\n",
    "        # if(torch.isnan(batch_x).any() or torch.isinf(batch_x).any() or torch.isnan(batch_y).any() or torch.isinf(batch_y).any()):\n",
    "        #     print(\"hi\")\n",
    "        # print(torch.isnan(batch_x).any(), torch.isinf(batch_x).any())\n",
    "        # print(torch.isnan(batch_y).any(), torch.isinf(batch_y).any())\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(batch_x)\n",
    "        # print(y_pred)\n",
    "        loss = criterion(y_pred.squeeze(), batch_y)\n",
    "        # print(y_pred.squeeze())\n",
    "        # print(batch_y)\n",
    "        # print(loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{EPOCHS}], Loss: {epoch_loss/len(train_loader):.6f}')\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell id=\"#VSC-cf177f81\" language=\"python\">\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred_test = model(X_test).squeeze().cpu().numpy()\n",
    "\n",
    "# Inverse transform predictions\n",
    "# y_pred_test_rescaled = scaler.inverse_transform(y_pred_test.reshape(-1, 1))\n",
    "# y_test_rescaled = scaler.inverse_transform(y_test.numpy().reshape(-1, 1))\n",
    "\n",
    "y_pred_test_rescaled = y_pred_test.reshape(-1, 1)\n",
    "y_test_rescaled = y_test.cpu().numpy().reshape(-1, 1)\n",
    "\n",
    "# Plot results\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(data.index[-500:], y_test_rescaled, label='Actual')\n",
    "plt.plot(data.index[-500:], y_pred_test_rescaled, label='Predicted')\n",
    "plt.legend()\n",
    "plt.title(\"LSTM Model Predictions on testing set\")\n",
    "plt.show()\n",
    "\n",
    "print(f\"r2 score {r2_score(y_test_rescaled, y_pred_test_rescaled):.4f}\")\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell id=\"#VSC-9d54df76\" language=\"markdown\">\n",
    "ARIMA with no weather\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell id=\"#VSC-829afeaa\" language=\"python\">\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "df_sugar = corn_data.dropna()\n",
    "\n",
    "split_index = int(len(df_sugar)*0.8)\n",
    "df_train = df_sugar[:split_index]['Log_Return_Shift']\n",
    "df_test = df_sugar[split_index:]['Log_Return_Shift']\n",
    "\n",
    "# Perform ADF test to check for stationarity\n",
    "result = adfuller(df_sugar['Log_Return_Shift'])\n",
    "print(f\"ADF Statistic: {result[0]}\")\n",
    "print(f\"p-value: {result[1]}\")\n",
    "\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell id=\"#VSC-908d0f98\" language=\"python\">\n",
    "# Fit a SARIMA model with seasonal order (p, d, q, s)\n",
    "model = SARIMAX(df_train, order=(30, 1, 1), seasonal_order=(0, 1, 0, 12))  # (p, d, q, s)\n",
    "model_fit = model.fit()\n",
    "\n",
    "# Summary of the ARIMA model\n",
    "print(model_fit.summary())\n",
    "\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell id=\"#VSC-9f7e3368\" language=\"markdown\">\n",
    "ARIMA with weather\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell id=\"#VSC-cfa506ea\" language=\"python\">\n",
    "# Make predictions (forecast) on the same dataset (for simplicity, we're using the same data for both)\n",
    "forecast_steps = 40 # Number of periods you want to predict\n",
    "forecast = model_fit.forecast(steps=forecast_steps)\n",
    "\n",
    "# Visualizing the forecast\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(df_test[:forecast_steps], label='Historical Data')\n",
    "plt.plot(df_test.index[:forecast_steps], forecast, label='Forecast', color='red')\n",
    "plt.title('ARIMA Forecast')\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell id=\"#VSC-73d90285\" language=\"python\">\n",
    "from sklearn.metrics import root_mean_squared_error as rmse\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "days = [5,10,15,20,25,30]\n",
    "for day in days:\n",
    "    print(f'Days = {day}')\n",
    "    print('Standard deviaiton of test data:', df_test[:day].std())\n",
    "    print('Root MSE:', rmse(df_test[:day],forecast[:day]))\n",
    "    print('R^2 score:', r2_score(df_test[:day],forecast[:day]))\n",
    "    print()\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell id=\"#VSC-1c5180ef\" language=\"python\">\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "df_sugar = corn_data_with_weather\n",
    "\n",
    "split_index = int(len(df_sugar)*0.8)\n",
    "df_train = df_sugar[:split_index]['Log_Return_Shift']\n",
    "df_test = df_sugar[split_index:]['Log_Return_Shift']\n",
    "\n",
    "# Perform ADF test to check for stationarity\n",
    "result = adfuller(df_sugar['Log_Return_Shift'])\n",
    "print(f\"ADF Statistic: {result[0]}\")\n",
    "print(f\"p-value: {result[1]}\")\n",
    "\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell id=\"#VSC-e06f60ac\" language=\"python\">\n",
    "# Fit a SARIMA model with seasonal order (p, d, q, s)\n",
    "model = SARIMAX(df_train, order=(30, 1, 1), seasonal_order=(0, 1, 0, 12))  # (p, d, q, s)\n",
    "model_fit = model.fit()\n",
    "\n",
    "# Summary of the ARIMA model\n",
    "print(model_fit.summary())\n",
    "\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell id=\"#VSC-4611a58d\" language=\"python\">\n",
    "# Make predictions (forecast) on the same dataset (for simplicity, we're using the same data for both)\n",
    "forecast_steps = 40 # Number of periods you want to predict\n",
    "forecast = model_fit.forecast(steps=forecast_steps)\n",
    "\n",
    "# Visualizing the forecast\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(df_test[:forecast_steps], label='Historical Data')\n",
    "plt.plot(df_test.index[:forecast_steps], forecast, label='Forecast', color='red')\n",
    "plt.title('ARIMA Forecast')\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell id=\"#VSC-21a162d1\" language=\"python\">\n",
    "from sklearn.metrics import root_mean_squared_error as rmse\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "days = [5,10,15,20,25,30]\n",
    "for day in days:\n",
    "    print(f'Days = {day}')\n",
    "    print('Standard deviaiton of test data:', df_test[:day].std())\n",
    "    print('Root MSE:', rmse(df_test[:day],forecast[:day]))\n",
    "    print('R^2 score:', r2_score(df_test[:day],forecast[:day]))\n",
    "    print()\n",
    "</VSCode.Cell>\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
