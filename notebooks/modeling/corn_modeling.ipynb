{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3963ccdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from src.data.preprocess import extend_market_data\n",
    "from src.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd9a6a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dl = DataLoader()\n",
    "\n",
    "# Define ticker symbols for corn and ethanol futures\n",
    "corn_ticker = \"ZC=F\"   # Corn Futures (CBOT)\n",
    "ethanol_ticker = \"ZS=F\"  # Ethanol Futures (NYMEX)\n",
    "corn = yf.Ticker(corn_ticker)\n",
    "ethanol = yf.Ticker(ethanol_ticker)\n",
    "corn_data = corn.history(start =\"2014-01-01\", end =\"2024-12-31\")\n",
    "ethanol_data = ethanol.history(start =\"2014-01-01\", end =\"2024-12-31\")\n",
    "\n",
    "corn_data = extend_market_data(corn_data)\n",
    "ethanol_data = extend_market_data(ethanol_data)\n",
    "\n",
    "features = list(corn_data.columns)\n",
    "features.remove('expiry')\n",
    "corn_data = corn_data[features]\n",
    "\n",
    "production_state_raw = dl.get_production_data(\"CORN\", 2014, national_level=False, raw=True)\n",
    "\n",
    "year = 2024\n",
    "production_state = production_state_raw[\n",
    "    (production_state_raw.unit_desc.isin(['BU', 'PCT BY TYPE'])) &\n",
    "    (production_state_raw.reference_period_desc == 'YEAR') &\n",
    "    (production_state_raw.year == year)\n",
    "][['state_alpha', 'state_name', 'Value', 'unit_desc']]\n",
    "\n",
    "production_state['Value'] = production_state['Value'].str.replace(',', '', regex=True)\n",
    "production_state['Value'] = pd.to_numeric(production_state['Value'], errors='coerce')\n",
    "\n",
    "\n",
    "winter_production_state = production_state.sort_values('Value', ascending=False)\n",
    "threshold = 48000000\n",
    "major = winter_production_state[winter_production_state['Value'] >= threshold]\n",
    "other = winter_production_state[winter_production_state['Value'] < threshold]\n",
    "other_sum = other.Value.sum()\n",
    "\n",
    "\n",
    "SRW_states_of_interest = winter_production_state[:9][['state_name', 'Value']]\n",
    "SRW_states_of_interest['weight'] = SRW_states_of_interest['Value'] / SRW_states_of_interest['Value'].sum()\n",
    "\n",
    "condition_state_raw = pd.concat(dl.get_condition_data('CORN', 2014, exact_year = year, national_level=False, raw=True) for year in range(2014,2026))\n",
    "\n",
    "\n",
    "condition_state_raw['year'] = pd.to_numeric(condition_state_raw['year'])\n",
    "condition_state_raw['Value'] = pd.to_numeric(condition_state_raw['Value'], errors='coerce')\n",
    "raw_data = condition_state_raw[\n",
    "    (condition_state_raw.state_name.isin(SRW_states_of_interest['state_name']))\n",
    "]\n",
    "condition_state = raw_data.pivot(index=['week_ending', 'year', 'state_name', 'end_code'], columns='unit_desc', values='Value').reset_index()\n",
    "condition_state.rename(columns={'end_code': 'week_number', 'week_ending': 'date'}, inplace=True)\n",
    "condition_state.date = pd.to_datetime(condition_state.date)\n",
    "\n",
    "# 1. Filter only SRW states\n",
    "condition_srw = condition_state[condition_state['state_name'].isin(SRW_states_of_interest['state_name'])]\n",
    "\n",
    "# 2. Merge weights into condition data\n",
    "condition_srw = condition_srw.merge(SRW_states_of_interest, on='state_name', how='left')\n",
    "\n",
    "# 3. Compute weighted condition percentages\n",
    "conditions = ['PCT EXCELLENT', 'PCT GOOD', 'PCT FAIR', 'PCT POOR', 'PCT VERY POOR']\n",
    "for col in conditions:\n",
    "    condition_srw[col] = pd.to_numeric(condition_srw[col], errors='coerce')\n",
    "    condition_srw[f'{col}_weighted'] = condition_srw[col] * condition_srw['weight']\n",
    "\n",
    "# 4. Aggregate weekly by year\n",
    "weekly_national = (\n",
    "    condition_srw\n",
    "    .groupby(['date'])[[f'{c}_weighted' for c in conditions]]\n",
    "    .sum()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "weekly_national['sum'] = (\n",
    "    weekly_national['PCT EXCELLENT_weighted'] +\n",
    "    weekly_national['PCT GOOD_weighted'] +\n",
    "    weekly_national['PCT FAIR_weighted'] +\n",
    "    weekly_national['PCT POOR_weighted'] +\n",
    "    weekly_national['PCT VERY POOR_weighted']\n",
    ")\n",
    "\n",
    "df = weekly_national[weekly_national['sum']>=80].copy()\n",
    "for condition in conditions:\n",
    "    df[f'{condition}_weighted'] = df[f'{condition}_weighted'] * 100 / df['sum']\n",
    "\n",
    "\n",
    "df['Good'] = df['PCT EXCELLENT_weighted'] + df['PCT GOOD_weighted']\n",
    "df.rename(columns={'date': 'Date'}, inplace=True)\n",
    "merged = pd.merge(df, corn_data, how='outer', on='Date')\n",
    "merged.sort_values('Date', inplace=True)\n",
    "merged.ffill(inplace=True)\n",
    "merged['next_day_increment'] = merged['Close'].shift(-1) - merged['Close']\n",
    "merged['next_3day_increment'] = merged['Close'].shift(-3) - merged['Close']\n",
    "merged['next_7day_increment'] = merged['Close'].shift(-7) - merged['Close']\n",
    "data = merged[merged.Date.isin(df.Date)][['Date', 'Good', 'Close', 'next_day_increment', 'next_3day_increment', 'next_7day_increment']].dropna()\n",
    "data['condition_increment'] = data['Good']-data['Good'].shift(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dcfdbe65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from src.data.preprocess import extend_market_data\n",
    "\n",
    "# Define ticker symbols for corn and ethanol futures\n",
    "corn_ticker = \"ZC=F\"   # Corn Futures (CBOT)\n",
    "ethanol_ticker = \"ZS=F\"  # Ethanol Futures (NYMEX)\n",
    "corn = yf.Ticker(corn_ticker)\n",
    "ethanol = yf.Ticker(ethanol_ticker)\n",
    "corn_data = corn.history(start =\"2014-01-01\", end =\"2024-12-31\")\n",
    "ethanol_data = ethanol.history(start =\"2014-01-01\", end =\"2024-12-31\")\n",
    "\n",
    "corn_data = extend_market_data(corn_data)\n",
    "ethanol_data = extend_market_data(ethanol_data)\n",
    "\n",
    "features = list(corn_data.columns)\n",
    "features.remove('expiry')\n",
    "corn_data = corn_data[features]\n",
    "\n",
    "features = list(ethanol_data.columns)\n",
    "features.remove('expiry')\n",
    "ethanol_data = ethanol_data[features]\n",
    "\n",
    "if merged.index.name != 'Date':\n",
    "    merged = merged.set_index('Date')\n",
    "merged = merged.drop(columns = [\"next_day_increment\", \"next_3day_increment\", \"next_7day_increment\"])\n",
    "\n",
    "corn_data[\"Log_Return_Shift\"] = corn_data[\"Log_Return\"].shift(-1)\n",
    "\n",
    "from datetime import datetime\n",
    "# start_date2231 = datetime.fromisoformat(\"2014-01-02\")\n",
    "import shelve\n",
    "with shelve.open('feat_dict.db') as features:\n",
    "\n",
    "    # Convert your features dict to a DataFrame\n",
    "    features_df = pd.DataFrame.from_dict(features, orient='index')\n",
    "    features_df.index = pd.to_datetime(features_df.index)\n",
    "    def create_shifted_features(features_df, lags):\n",
    "        shifted_dfs = []\n",
    "        for lag in lags:\n",
    "            shifted = features_df.shift(lag)\n",
    "            shifted.columns = [f\"{col}_lag_{lag}\" for col in features_df.columns]\n",
    "            shifted_dfs.append(shifted)\n",
    "    \n",
    "        # Concatenate all lagged features side-by-side\n",
    "        return pd.concat(shifted_dfs, axis=1)\n",
    "\n",
    "    lags = [2**i for i in range(1)]\n",
    "    lagged_features = create_shifted_features(features_df, lags)\n",
    "    # print(lagged_features.columns)\n",
    "    features_df = features_df.join(lagged_features, how = 'inner')\n",
    "    # Combine with your existing df\n",
    "    corn_data_with_weather = corn_data.join(features_df, how='left')\n",
    "    corn_data_with_weather = corn_data_with_weather.dropna()\n",
    "merged = merged.drop(columns=corn_data_with_weather.columns.intersection(merged.columns))\n",
    "merged = merged.dropna()\n",
    "corn_data_with_weather = corn_data_with_weather.dropna()\n",
    "corn_data_with_weather = corn_data_with_weather.join(merged, how='inner')\n",
    "# from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "# poly_array = poly.fit_transform(corn_data_with_weather)\n",
    "# poly_feature_names = poly.get_feature_names_out(corn_data_with_weather.columns)\n",
    "# corn_data_with_weather = pd.DataFrame(poly_array, columns=poly_feature_names, index=corn_data_with_weather.index)\n",
    "\n",
    "corn_data_train = corn_data[:datetime.fromisoformat(\"2023-12-31\")]\n",
    "corn_data_test = corn_data[datetime.fromisoformat(\"2023-12-31\"):]\n",
    "corn_data_with_weather_train = corn_data_with_weather[:datetime.fromisoformat(\"2023-12-31\")]\n",
    "corn_data_with_weather_test = corn_data_with_weather[datetime.fromisoformat(\"2023-12-31\"):]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5284f53",
   "metadata": {},
   "source": [
    "## Linear Regression with no weather data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "44ba0285",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R² Score Linear: -1.3524\n",
      "R² Score Linear: -0.0904\n",
      "R² Score Linear: -0.0113\n",
      "R² Score Linear: -0.1422\n",
      "R² Score Linear: -0.0637\n",
      "R² Score Linear: -0.0360\n",
      "R² Score Linear: -0.3425\n",
      "R² Score Linear: -0.1347\n",
      "R² Score Linear: -0.1005\n",
      "R² Score Linear: -0.0516\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "corn_data2 = corn_data_train.dropna()\n",
    "\n",
    "lin = LinearRegression()\n",
    "features = list(corn_data.columns)\n",
    "features.remove('Log_Return_Shift')\n",
    "X = corn_data2[list(features)]\n",
    "y = corn_data2[\"Log_Return_Shift\"]\n",
    "tss = TimeSeriesSplit(n_splits = 10)\n",
    "r2 = 0\n",
    "for i, (train_index, test_index) in enumerate(tss.split(X)):\n",
    "    X_train = X.iloc[train_index]\n",
    "    X_test = X.iloc[test_index]\n",
    "    y_train = y.iloc[train_index]\n",
    "    y_test = y.iloc[test_index]\n",
    "    lin.fit(X_train,y_train)\n",
    "    predlin = lin.predict(X_test)\n",
    "    r2 = r2_score(y_test, predlin)\n",
    "    print(f\"R² Score Linear: {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a78fb8b",
   "metadata": {},
   "source": [
    "## Linear regression with weather data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d422a67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R² Score Linear: -2.5325\n",
      "R² Score Linear: -1.2542\n",
      "R² Score Linear: -0.2445\n",
      "R² Score Linear: -0.2905\n",
      "R² Score Linear: -0.0121\n",
      "R² Score Linear: -0.0556\n",
      "R² Score Linear: -1.5235\n",
      "R² Score Linear: -0.2021\n",
      "R² Score Linear: -0.1816\n",
      "R² Score Linear: -0.0670\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "features = list(corn_data_with_weather.columns)\n",
    "features.remove('Log_Return_Shift')\n",
    "\n",
    "corn_data2 = corn_data_with_weather_train.dropna()\n",
    "\n",
    "lin_weather = LinearRegression()\n",
    "# features = [\"average_temperature_distribution_weighted_kurtosis\"]\n",
    "X = corn_data_with_weather_train[list(features)]\n",
    "y = corn_data_with_weather_train[\"Log_Return_Shift\"]\n",
    "tss = TimeSeriesSplit(n_splits = 10)\n",
    "r2 = 0\n",
    "for i, (train_index, test_index) in enumerate(tss.split(X)):\n",
    "    X_train = X.iloc[train_index]\n",
    "    X_test = X.iloc[test_index]\n",
    "    y_train = y.iloc[train_index]\n",
    "    y_test = y.iloc[test_index]\n",
    "    lin_weather.fit(X_train,y_train)\n",
    "    predlin = lin_weather.predict(X_test)\n",
    "    r2 = r2_score(y_test, predlin)\n",
    "    print(f\"R² Score Linear: {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4abc44ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def obj(params):\n",
    "\n",
    "    if 'n_estimators' in params.keys():\n",
    "        params['n_estimators'] = int(params['n_estimators'])\n",
    "    if 'max_depth' in params.keys():\n",
    "        params['max_depth'] = int(params['max_depth'])\n",
    "    \n",
    "    \n",
    "    features = list(corn_data.columns)\n",
    "    features.remove('Log_Return_Shift')\n",
    "    \n",
    "    corn_data2 = corn_data_train.dropna()\n",
    "    X = corn_data2[features]\n",
    "    y = corn_data2[\"Log_Return_Shift\"]\n",
    "    tss = TimeSeriesSplit(n_splits = 5)\n",
    "    r2 = 0\n",
    "    for i, (train_index, test_index) in enumerate(tss.split(X)):\n",
    "        \n",
    "        X_train = X.iloc[train_index]\n",
    "        X_test = X.iloc[test_index]\n",
    "        y_train = y.iloc[train_index]\n",
    "        y_test = y.iloc[test_index]\n",
    "        \n",
    "        dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "        dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "        \n",
    "        xgb_model = xgb.XGBRegressor(**params)\n",
    "        xgb_model.fit(X_train, y_train)\n",
    "        y_pred = xgb_model.predict(X_test)\n",
    "        sc = r2_score(y_test, y_pred)\n",
    "        r2 += r2_score(y_test, y_pred)\n",
    "    return -r2\n",
    "\n",
    "def obj_with_weather(params):\n",
    "\n",
    "    if 'n_estimators' in params.keys():\n",
    "        params['n_estimators'] = int(params['n_estimators'])\n",
    "    if 'max_depth' in params.keys():\n",
    "        params['max_depth'] = int(params['max_depth'])\n",
    "    \n",
    "    \n",
    "    features = list(corn_data_with_weather_train.columns)\n",
    "    features.remove('Log_Return_Shift')\n",
    "\n",
    "    corn_data2 = corn_data_with_weather_train.dropna()\n",
    "    X = corn_data2[features]\n",
    "    y = corn_data2[\"Log_Return_Shift\"]\n",
    "    tss = TimeSeriesSplit(n_splits = 5)\n",
    "    r2 = 0\n",
    "    for i, (train_index, test_index) in enumerate(tss.split(X)):\n",
    "        \n",
    "        X_train = X.iloc[train_index]\n",
    "        X_test = X.iloc[test_index]\n",
    "        y_train = y.iloc[train_index]\n",
    "        y_test = y.iloc[test_index]\n",
    "        \n",
    "        dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "        dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "        \n",
    "        xgb_model = xgb.XGBRegressor(**params)\n",
    "        xgb_model.fit(X_train, y_train)\n",
    "        y_pred = xgb_model.predict(X_test)\n",
    "        sc = r2_score(y_test, y_pred)\n",
    "        r2 += r2_score(y_test, y_pred)\n",
    "    return -r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e53aac9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/thwang/PycharmProjects/ErdosAgriDerivPredict/.venv/lib/python3.12/site-packages/hyperopt/atpe.py:19: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\n"
     ]
    }
   ],
   "source": [
    "from hyperopt import fmin, tpe, hp\n",
    "# Define the hyperparameter space\n",
    "paramspace = {\n",
    "    'objective': 'reg:squarederror',  # Regression\n",
    "    'eval_metric': 'rmse',\n",
    "    'n_estimators': hp.quniform('n_estimators', 10,200,10),\n",
    "    'max_depth': hp.quniform('max_depth', 2, 8, 1),\n",
    "    'learning_rate': hp.loguniform('learning_rate', -5, -1),\n",
    "    'min_child_weight': hp.quniform('min_child_weight', 1, 100, 9),\n",
    "    'gamma': hp.quniform('gamma', 0, 5, 1),\n",
    "    'subsample': hp.uniform('subsample', .5, 1)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a265c320",
   "metadata": {},
   "source": [
    "## xgboost with no weather data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "91298912",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [08:16<00:00,  2.01trial/s, best loss: -0.059122995983845406]\n"
     ]
    }
   ],
   "source": [
    "best_params_no_weather = fmin(obj, paramspace, algo=tpe.suggest, max_evals=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08cd461c",
   "metadata": {},
   "source": [
    "## xgboost with weather data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "921cc860",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [04:40<00:00,  3.57trial/s, best loss: 0.010587944356914125]\n"
     ]
    }
   ],
   "source": [
    "best_params_with_weather = fmin(obj_with_weather, paramspace, algo=tpe.suggest, max_evals=1000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1707f457",
   "metadata": {},
   "source": [
    "XGBoost with no weather data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564f5151",
   "metadata": {},
   "outputs": [],
   "source": [
    "obj(best_params_no_weather)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9475558a",
   "metadata": {},
   "source": [
    "XGBoost with weather data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aad1d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_with_weather(best_params_with_weather)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efcef8a9",
   "metadata": {},
   "source": [
    "## Neural network with no weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0682d725",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = corn_data.dropna()\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler on the training data and transform it.\n",
    "scaled = scaler.fit_transform(combined_df)\n",
    "\n",
    "# Convert the NumPy arrays back into DataFrames.\n",
    "scaled_df = pd.DataFrame(scaled, index=combined_df.index, columns=combined_df.columns)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Set device for PyTorch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Prepare target and predictors as NumPy arrays.\n",
    "# combined_df should contain the target 'Log_Return_Shift' and predictors.\n",
    "y = scaled_df['Log_Return_Shift'].values\n",
    "X = scaled_df.drop(columns=['Log_Return_Shift']).values\n",
    "\n",
    "# Define a simple feed-forward neural network model.\n",
    "class FeedForwardNN(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(FeedForwardNN, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# Training function with early stopping.\n",
    "def train_model(model, optimizer, criterion, X_train, y_train, X_val, y_val,\n",
    "                num_epochs=100, batch_size=32, patience=10):\n",
    "    model.train()\n",
    "    n_train = X_train.shape[0]\n",
    "    best_val_loss = np.inf\n",
    "    epochs_no_improve = 0\n",
    "    best_model_state = None\n",
    "\n",
    "    # Convert all training and validation data to tensors.\n",
    "    X_train_tensor = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "    y_train_tensor = torch.tensor(y_train, dtype=torch.float32).view(-1, 1).to(device)\n",
    "    X_val_tensor = torch.tensor(X_val, dtype=torch.float32).to(device)\n",
    "    y_val_tensor = torch.tensor(y_val, dtype=torch.float32).view(-1, 1).to(device)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        permutation = torch.randperm(n_train)\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        # Mini-batch training.\n",
    "        for i in range(0, n_train, batch_size):\n",
    "            optimizer.zero_grad()\n",
    "            indices = permutation[i:i+batch_size]\n",
    "            batch_x = X_train_tensor[indices]\n",
    "            batch_y = y_train_tensor[indices]\n",
    "            outputs = model(batch_x)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item() * batch_x.size(0)\n",
    "        \n",
    "        epoch_loss /= n_train\n",
    "\n",
    "        # Evaluate on validation data.\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_outputs = model(X_val_tensor)\n",
    "            val_loss = criterion(val_outputs, y_val_tensor).item()\n",
    "        \n",
    "        # Early stopping check.\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model_state = model.state_dict()\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "\n",
    "    # Restore best model state.\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Set up TimeSeriesSplit for cross-validation.\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "nn_errors = []  # To store (RMSE, R^2) for each fold.\n",
    "\n",
    "for fold, (train_index, test_index) in enumerate(tscv.split(combined_df)):\n",
    "    # Get fold data.\n",
    "    train_data = combined_df.iloc[train_index]\n",
    "    test_data = combined_df.iloc[test_index]\n",
    "    \n",
    "    X_train = train_data.drop(columns=['Log_Return_Shift']).values\n",
    "    y_train = train_data['Log_Return_Shift'].values\n",
    "    X_test = test_data.drop(columns=['Log_Return_Shift']).values\n",
    "    y_test = test_data['Log_Return_Shift'].values\n",
    "    \n",
    "    # Further split training data to have a validation set (e.g., 80/20 split).\n",
    "    split_idx = int(0.8 * X_train.shape[0])\n",
    "    X_train_part, X_val = X_train[:split_idx], X_train[split_idx:]\n",
    "    y_train_part, y_val = y_train[:split_idx], y_train[split_idx:]\n",
    "    \n",
    "    input_dim = X_train.shape[1]\n",
    "    model = FeedForwardNN(input_dim).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    # Train with early stopping.\n",
    "    model = train_model(model, optimizer, criterion,\n",
    "                        X_train_part, y_train_part, X_val, y_val,\n",
    "                        num_epochs=100, batch_size=32, patience=10)\n",
    "    \n",
    "    # Evaluate on the test set.\n",
    "    model.eval()\n",
    "    X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "    with torch.no_grad():\n",
    "        test_pred = model(X_test_tensor).cpu().numpy().flatten()\n",
    "    \n",
    "    nn_rmse = np.sqrt(mean_squared_error(y_test, test_pred))\n",
    "    nn_r2 = r2_score(y_test, test_pred)\n",
    "    nn_errors.append((nn_rmse, nn_r2))\n",
    "    \n",
    "    print(f\"Fold {fold+1}: NN RMSE: {nn_rmse:.4f}, NN R^2: {nn_r2:.4f}\")\n",
    "\n",
    "# Compute average metrics across folds.\n",
    "nn_errors_arr = np.array(nn_errors)\n",
    "avg_nn_rmse, avg_nn_r2 = nn_errors_arr.mean(axis=0)\n",
    "print(\"\\nAverage Neural Network Metrics (PyTorch):\")\n",
    "print(f\"  Average RMSE: {avg_nn_rmse:.4f}\")\n",
    "print(f\"  Average R^2: {avg_nn_r2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce65ae3",
   "metadata": {},
   "source": [
    "## Neural network with weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44313d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = corn_data_with_weather.dropna()\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler on the training data and transform it.\n",
    "scaled = scaler.fit_transform(combined_df)\n",
    "\n",
    "# Convert the NumPy arrays back into DataFrames.\n",
    "scaled_df = pd.DataFrame(scaled, index=combined_df.index, columns=combined_df.columns)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Set device for PyTorch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Prepare target and predictors as NumPy arrays.\n",
    "# combined_df should contain the target 'Log_Return_Shift' and predictors.\n",
    "y = scaled_df['Log_Return_Shift'].values\n",
    "X = scaled_df.drop(columns=['Log_Return_Shift']).values\n",
    "\n",
    "# Define a simple feed-forward neural network model.\n",
    "class FeedForwardNN(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(FeedForwardNN, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# Training function with early stopping.\n",
    "def train_model(model, optimizer, criterion, X_train, y_train, X_val, y_val,\n",
    "                num_epochs=100, batch_size=32, patience=10):\n",
    "    model.train()\n",
    "    n_train = X_train.shape[0]\n",
    "    best_val_loss = np.inf\n",
    "    epochs_no_improve = 0\n",
    "    best_model_state = None\n",
    "\n",
    "    # Convert all training and validation data to tensors.\n",
    "    X_train_tensor = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "    y_train_tensor = torch.tensor(y_train, dtype=torch.float32).view(-1, 1).to(device)\n",
    "    X_val_tensor = torch.tensor(X_val, dtype=torch.float32).to(device)\n",
    "    y_val_tensor = torch.tensor(y_val, dtype=torch.float32).view(-1, 1).to(device)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        permutation = torch.randperm(n_train)\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        # Mini-batch training.\n",
    "        for i in range(0, n_train, batch_size):\n",
    "            optimizer.zero_grad()\n",
    "            indices = permutation[i:i+batch_size]\n",
    "            batch_x = X_train_tensor[indices]\n",
    "            batch_y = y_train_tensor[indices]\n",
    "            outputs = model(batch_x)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item() * batch_x.size(0)\n",
    "        \n",
    "        epoch_loss /= n_train\n",
    "\n",
    "        # Evaluate on validation data.\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_outputs = model(X_val_tensor)\n",
    "            val_loss = criterion(val_outputs, y_val_tensor).item()\n",
    "        \n",
    "        # Early stopping check.\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model_state = model.state_dict()\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "\n",
    "    # Restore best model state.\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Set up TimeSeriesSplit for cross-validation.\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "nn_errors = []  # To store (RMSE, R^2) for each fold.\n",
    "\n",
    "for fold, (train_index, test_index) in enumerate(tscv.split(combined_df)):\n",
    "    # Get fold data.\n",
    "    train_data = combined_df.iloc[train_index]\n",
    "    test_data = combined_df.iloc[test_index]\n",
    "    \n",
    "    X_train = train_data.drop(columns=['Log_Return_Shift']).values\n",
    "    y_train = train_data['Log_Return_Shift'].values\n",
    "    X_test = test_data.drop(columns=['Log_Return_Shift']).values\n",
    "    y_test = test_data['Log_Return_Shift'].values\n",
    "    \n",
    "    # Further split training data to have a validation set (e.g., 80/20 split).\n",
    "    split_idx = int(0.8 * X_train.shape[0])\n",
    "    X_train_part, X_val = X_train[:split_idx], X_train[split_idx:]\n",
    "    y_train_part, y_val = y_train[:split_idx], y_train[split_idx:]\n",
    "    \n",
    "    input_dim = X_train.shape[1]\n",
    "    model = FeedForwardNN(input_dim).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    # Train with early stopping.\n",
    "    model = train_model(model, optimizer, criterion,\n",
    "                        X_train_part, y_train_part, X_val, y_val,\n",
    "                        num_epochs=100, batch_size=32, patience=10)\n",
    "    \n",
    "    # Evaluate on the test set.\n",
    "    model.eval()\n",
    "    X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "    with torch.no_grad():\n",
    "        test_pred = model(X_test_tensor).cpu().numpy().flatten()\n",
    "    \n",
    "    nn_rmse = np.sqrt(mean_squared_error(y_test, test_pred))\n",
    "    nn_r2 = r2_score(y_test, test_pred)\n",
    "    nn_errors.append((nn_rmse, nn_r2))\n",
    "    \n",
    "    print(f\"Fold {fold+1}: NN RMSE: {nn_rmse:.4f}, NN R^2: {nn_r2:.4f}\")\n",
    "\n",
    "# Compute average metrics across folds.\n",
    "nn_errors_arr = np.array(nn_errors)\n",
    "avg_nn_rmse, avg_nn_r2 = nn_errors_arr.mean(axis=0)\n",
    "print(\"\\nAverage Neural Network Metrics (PyTorch):\")\n",
    "print(f\"  Average RMSE: {avg_nn_rmse:.4f}\")\n",
    "print(f\"  Average R^2: {avg_nn_r2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afdbdf70",
   "metadata": {},
   "source": [
    "## LSTM models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a122657",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "device = \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8864e030",
   "metadata": {},
   "source": [
    "LSTM with no weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb1a0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = corn_data.dropna()\n",
    "\n",
    "def create_sequences(data, seq_length, features):\n",
    "    scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "    data_scaled = scaler.fit_transform(data)\n",
    "    \n",
    "    features = list(data.columns)\n",
    "    features.remove(\"Log_Return_Shift\")\n",
    "    \n",
    "    feature_frame = data[features]\n",
    "    target_series = data[\"Log_Return_Shift\"]\n",
    "    sequences = []\n",
    "    targets = []\n",
    "    for i in range(len(data) - seq_length):\n",
    "        sequences.append(feature_frame.iloc[i:i+seq_length])\n",
    "        targets.append(target_series.iloc[i+seq_length])\n",
    "    return np.array(sequences), np.array(targets)\n",
    "\n",
    "\n",
    "\n",
    "SEQ_LENGTH = 60\n",
    "X, y = create_sequences(data, SEQ_LENGTH, features)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train, y_train = torch.tensor(X[:-500], dtype=torch.float32).to(device), torch.tensor(y[:-500], dtype=torch.float32).to(device)\n",
    "X_test, y_test = torch.tensor(X[-500:], dtype=torch.float32).to(device), torch.tensor(y[-500:], dtype=torch.float32).to(device)\n",
    "\n",
    "# print(X_train.shape[2])\n",
    "# Reshape for LSTM (batch_size, seq_length, num_features)\n",
    "X_train = X_train.view(-1, SEQ_LENGTH, X_train.shape[2])\n",
    "X_test = X_test.view(-1, SEQ_LENGTH, X_train.shape[2])\n",
    "\n",
    "# Create DataLoader\n",
    "BATCH_SIZE = 32\n",
    "train_loader = torch.utils.data.DataLoader(TensorDataset(X_train, y_train), batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(TensorDataset(X_test, y_test), batch_size=BATCH_SIZE, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d4e0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=64, num_layers=2, output_dim=1):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        # print(lstm_out)\n",
    "        out = self.fc(lstm_out[:, -1, :])  # Take last output from LSTM\n",
    "        # print(out)\n",
    "        return out\n",
    "\n",
    "# Initialize Model\n",
    "model = LSTMModel(input_dim = X_train.shape[2]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92d7462",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Loss and Optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training Loop\n",
    "EPOCHS = 100\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for batch_x, batch_y in train_loader:\n",
    "        # if(torch.isnan(batch_x).any() or torch.isinf(batch_x).any() or torch.isnan(batch_y).any() or torch.isinf(batch_y).any()):\n",
    "        #     print(\"hi\")\n",
    "        # print(torch.isnan(batch_x).any(), torch.isinf(batch_x).any())\n",
    "        # print(torch.isnan(batch_y).any(), torch.isinf(batch_y).any())\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(batch_x)\n",
    "        # print(y_pred)\n",
    "        loss = criterion(y_pred.squeeze(), batch_y)\n",
    "        # print(y_pred.squeeze())\n",
    "        # print(batch_y)\n",
    "        # print(loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{EPOCHS}], Loss: {epoch_loss/len(train_loader):.6f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5e7451",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred_test = model(X_test).squeeze().cpu().numpy()\n",
    "\n",
    "# Inverse transform predictions\n",
    "# y_pred_test_rescaled = scaler.inverse_transform(y_pred_test.reshape(-1, 1))\n",
    "# y_test_rescaled = scaler.inverse_transform(y_test.numpy().reshape(-1, 1))\n",
    "\n",
    "y_pred_test_rescaled = y_pred_test.reshape(-1, 1)\n",
    "y_test_rescaled = y_test.cpu().numpy().reshape(-1, 1)\n",
    "\n",
    "# Plot results\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(data.index[-500:], y_test_rescaled, label='Actual')\n",
    "plt.plot(data.index[-500:], y_pred_test_rescaled, label='Predicted')\n",
    "plt.legend()\n",
    "plt.title(\"LSTM Model Predictions on testing set\")\n",
    "plt.show()\n",
    "\n",
    "print(f\"r2 score {r2_score(y_test_rescaled, y_pred_test_rescaled):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15f325e",
   "metadata": {},
   "source": [
    "LSTM with weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f43d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = corn_data_with_weather.dropna()\n",
    "\n",
    "def create_sequences(data, seq_length, features):\n",
    "    scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "    data_scaled = scaler.fit_transform(data)\n",
    "    \n",
    "    features = list(data.columns)\n",
    "    features.remove(\"Log_Return_Shift\")\n",
    "    \n",
    "    feature_frame = data[features]\n",
    "    target_series = data[\"Log_Return_Shift\"]\n",
    "    sequences = []\n",
    "    targets = []\n",
    "    for i in range(len(data) - seq_length):\n",
    "        sequences.append(feature_frame.iloc[i:i+seq_length])\n",
    "        targets.append(target_series.iloc[i+seq_length])\n",
    "    return np.array(sequences), np.array(targets)\n",
    "\n",
    "\n",
    "\n",
    "SEQ_LENGTH = 60\n",
    "X, y = create_sequences(data, SEQ_LENGTH, features)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train, y_train = torch.tensor(X[:-500], dtype=torch.float32).to(device), torch.tensor(y[:-500], dtype=torch.float32).to(device)\n",
    "X_test, y_test = torch.tensor(X[-500:], dtype=torch.float32).to(device), torch.tensor(y[-500:], dtype=torch.float32).to(device)\n",
    "\n",
    "# print(X_train.shape[2])\n",
    "# Reshape for LSTM (batch_size, seq_length, num_features)\n",
    "X_train = X_train.view(-1, SEQ_LENGTH, X_train.shape[2])\n",
    "X_test = X_test.view(-1, SEQ_LENGTH, X_train.shape[2])\n",
    "\n",
    "# Create DataLoader\n",
    "BATCH_SIZE = 32\n",
    "train_loader = torch.utils.data.DataLoader(TensorDataset(X_train, y_train), batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(TensorDataset(X_test, y_test), batch_size=BATCH_SIZE, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc13d705",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=64, num_layers=2, output_dim=1):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        # print(lstm_out)\n",
    "        out = self.fc(lstm_out[:, -1, :])  # Take last output from LSTM\n",
    "        # print(out)\n",
    "        return out\n",
    "\n",
    "# Initialize Model\n",
    "model = LSTMModel(input_dim = X_train.shape[2]).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8209c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Loss and Optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training Loop\n",
    "EPOCHS = 100\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for batch_x, batch_y in train_loader:\n",
    "        # if(torch.isnan(batch_x).any() or torch.isinf(batch_x).any() or torch.isnan(batch_y).any() or torch.isinf(batch_y).any()):\n",
    "        #     print(\"hi\")\n",
    "        # print(torch.isnan(batch_x).any(), torch.isinf(batch_x).any())\n",
    "        # print(torch.isnan(batch_y).any(), torch.isinf(batch_y).any())\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(batch_x)\n",
    "        # print(y_pred)\n",
    "        loss = criterion(y_pred.squeeze(), batch_y)\n",
    "        # print(y_pred.squeeze())\n",
    "        # print(batch_y)\n",
    "        # print(loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{EPOCHS}], Loss: {epoch_loss/len(train_loader):.6f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84eb21d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred_test = model(X_test).squeeze().cpu().numpy()\n",
    "\n",
    "# Inverse transform predictions\n",
    "# y_pred_test_rescaled = scaler.inverse_transform(y_pred_test.reshape(-1, 1))\n",
    "# y_test_rescaled = scaler.inverse_transform(y_test.numpy().reshape(-1, 1))\n",
    "\n",
    "y_pred_test_rescaled = y_pred_test.reshape(-1, 1)\n",
    "y_test_rescaled = y_test.cpu().numpy().reshape(-1, 1)\n",
    "\n",
    "# Plot results\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(data.index[-500:], y_test_rescaled, label='Actual')\n",
    "plt.plot(data.index[-500:], y_pred_test_rescaled, label='Predicted')\n",
    "plt.legend()\n",
    "plt.title(\"LSTM Model Predictions on testing set\")\n",
    "plt.show()\n",
    "\n",
    "print(f\"r2 score {r2_score(y_test_rescaled, y_pred_test_rescaled):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d123c4",
   "metadata": {},
   "source": [
    "ARIMA with no weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98469ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "df_sugar = corn_data.dropna()\n",
    "\n",
    "split_index = int(len(df_sugar)*0.8)\n",
    "df_train = df_sugar[:split_index]['Log_Return_Shift']\n",
    "df_test = df_sugar[split_index:]['Log_Return_Shift']\n",
    "\n",
    "# Perform ADF test to check for stationarity\n",
    "result = adfuller(df_sugar['Log_Return_Shift'])\n",
    "print(f\"ADF Statistic: {result[0]}\")\n",
    "print(f\"p-value: {result[1]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a736352",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Fit a SARIMA model with seasonal order (p, d, q, s)\n",
    "model = SARIMAX(df_train, order=(30, 1, 1), seasonal_order=(0, 1, 0, 12))  # (p, d, q, s)\n",
    "model_fit = model.fit()\n",
    "\n",
    "# Summary of the ARIMA model\n",
    "print(model_fit.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dcd37ac",
   "metadata": {},
   "source": [
    "ARIMA with weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48c8433",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Make predictions (forecast) on the same dataset (for simplicity, we're using the same data for both)\n",
    "forecast_steps = 40 # Number of periods you want to predict\n",
    "forecast = model_fit.forecast(steps=forecast_steps)\n",
    "\n",
    "# Visualizing the forecast\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(df_test[:forecast_steps], label='Historical Data')\n",
    "plt.plot(df_test.index[:forecast_steps], forecast, label='Forecast', color='red')\n",
    "plt.title('ARIMA Forecast')\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e484b3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import root_mean_squared_error as rmse\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "days = [5,10,15,20,25,30]\n",
    "for day in days:\n",
    "    print(f'Days = {day}')\n",
    "    print('Standard deviaiton of test data:', df_test[:day].std())\n",
    "    print('Root MSE:', rmse(df_test[:day],forecast[:day]))\n",
    "    print('R^2 score:', r2_score(df_test[:day],forecast[:day]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f683458c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "df_sugar = corn_data_with_weather\n",
    "\n",
    "split_index = int(len(df_sugar)*0.8)\n",
    "df_train = df_sugar[:split_index]['Log_Return_Shift']\n",
    "df_test = df_sugar[split_index:]['Log_Return_Shift']\n",
    "\n",
    "# Perform ADF test to check for stationarity\n",
    "result = adfuller(df_sugar['Log_Return_Shift'])\n",
    "print(f\"ADF Statistic: {result[0]}\")\n",
    "print(f\"p-value: {result[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909083d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Fit a SARIMA model with seasonal order (p, d, q, s)\n",
    "model = SARIMAX(df_train, order=(30, 1, 1), seasonal_order=(0, 1, 0, 12))  # (p, d, q, s)\n",
    "model_fit = model.fit()\n",
    "\n",
    "# Summary of the ARIMA model\n",
    "print(model_fit.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c1dd294",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Make predictions (forecast) on the same dataset (for simplicity, we're using the same data for both)\n",
    "forecast_steps = 40 # Number of periods you want to predict\n",
    "forecast = model_fit.forecast(steps=forecast_steps)\n",
    "\n",
    "# Visualizing the forecast\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(df_test[:forecast_steps], label='Historical Data')\n",
    "plt.plot(df_test.index[:forecast_steps], forecast, label='Forecast', color='red')\n",
    "plt.title('ARIMA Forecast')\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de2e96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import root_mean_squared_error as rmse\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "days = [5,10,15,20,25,30]\n",
    "for day in days:\n",
    "    print(f'Days = {day}')\n",
    "    print('Standard deviaiton of test data:', df_test[:day].std())\n",
    "    print('Root MSE:', rmse(df_test[:day],forecast[:day]))\n",
    "    print('R^2 score:', r2_score(df_test[:day],forecast[:day]))\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ErdosAgriDerivPredict",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
