{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from src.data import DataLoader\n",
    "from src.data.preprocess import extend_market_data"
   ],
   "id": "29754a2908568943"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "SRW = yf.Ticker(\"ZW=F\")\n",
    "SRW_data = SRW.history(start =\"2014-01-01\").drop(['Dividends', 'Stock Splits'], axis=1)\n",
    "market_data = extend_market_data(SRW_data)"
   ],
   "id": "b549f75fd45db5a5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "market_data",
   "id": "3c2920f7af431759"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "dl = DataLoader()\n",
    "production_raw = dl.get_production_data(\"WHEAT\", 2014, True, raw=True)"
   ],
   "id": "bb60db5a002e160a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "production_national = production_raw[\n",
    "    (production_raw.unit_desc == 'BU') &\n",
    "    (production_raw.short_desc == 'WHEAT - PRODUCTION, MEASURED IN BU') &\n",
    "    (production_raw.domain_desc == 'TOTAL')\n",
    "][['state_name', 'Value', 'unit_desc', 'year', 'source_desc', 'short_desc']]\n",
    "\n",
    "production_national['year'] = pd.to_numeric(production_national['year'])\n",
    "production_national['Value'] = production_national['Value'].str.replace(',', '', regex=True)\n",
    "production_national['Value'] = pd.to_numeric(production_national['Value'], errors='coerce')\n",
    "\n",
    "production_national = production_national.groupby(by ='year').agg({'Value': 'mean'})\n",
    "production_national.rename(columns={'Value': 'Production'}, inplace = True)\n",
    "production_national"
   ],
   "id": "4bb89017961b0360"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "production_state_raw = dl.get_production_data(\"WHEAT\", 2014, raw=True)\n",
    "production_state = production_state_raw[\n",
    "    (production_state_raw.unit_desc == 'BU') &\n",
    "    (production_state_raw.class_desc == 'WINTER') &\n",
    "#    ((production_state_raw.class_desc == 'WINTER') | (production_state_raw.class_desc == 'ALL CLASSES')) &\n",
    "    (production_state_raw.domaincat_desc == 'NOT SPECIFIED') &\n",
    "    (production_state_raw.short_desc == 'WHEAT, WINTER - PRODUCTION, MEASURED IN BU')\n",
    "][['state_name', 'Value', 'unit_desc', 'year', 'source_desc', 'class_desc', 'reference_period_desc', 'short_desc']]\n",
    "\n",
    "production_state['year'] = pd.to_numeric(production_state['year'])\n",
    "production_state['Value'] = production_state['Value'].str.replace(',', '', regex=True)\n",
    "production_state['Value'] = pd.to_numeric(production_state['Value'], errors='coerce')\n",
    "# production_state"
   ],
   "id": "58ce5a2214d3aad0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "production_2022 = production_state[(production_state.year == 2022)].groupby(by ='state_name').agg({'Value': 'mean'}).sort_values('Value', ascending=False).dropna()\n",
    "plt.pie(production_2022['Value'], labels=production_2022['Value'].index)"
   ],
   "id": "18dad11c301f7b0e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "states_of_interest = production_2022[:11].index.tolist()\n",
    "states_of_interest.remove('OTHER STATES')\n",
    "states_of_interest"
   ],
   "id": "7eab69cfb7e886be"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "stocks_national_raw = dl.get_stocks_data('WHEAT', 2014, True)\n",
    "stocks_national_raw['end_month'] = stocks_national_raw['end_month'].astype(int)\n",
    "stocks_national = stocks_national_raw[['year', 'end_month', 'WHEAT - STOCKS, MEASURED IN BU', 'WHEAT, OFF FARM - STOCKS, MEASURED IN BU', 'WHEAT, ON FARM - STOCKS, MEASURED IN BU']]\n",
    "# stocks_national.index.name = 'id'\n",
    "stocks_national = stocks_national.rename(columns={\n",
    "    'WHEAT - STOCKS, MEASURED IN BU': 'Total',\n",
    "    'WHEAT, ON FARM - STOCKS, MEASURED IN BU': 'ON_FARM',\n",
    "    'WHEAT, OFF FARM - STOCKS, MEASURED IN BU': 'OFF_FARM',\n",
    "})\n",
    "stocks_national"
   ],
   "id": "4847be2e80345b62"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "condition_state_raw = dl.get_condition_data('WHEAT', 2018, national_level=False, raw=True)\n",
    "\n",
    "condition_state_raw['year'] = pd.to_numeric(condition_state_raw['year'])\n",
    "# condition_national_raw['Value'] = condition_national_raw['Value'].str.replace(',', '', regex=True)\n",
    "condition_state_raw['Value'] = pd.to_numeric(condition_state_raw['Value'], errors='coerce')\n",
    "raw_data = condition_state_raw[condition_state_raw.class_desc == 'WINTER']\n",
    "\n",
    "condition_state = raw_data.pivot(index=['week_ending', 'year', 'state_name', 'end_code'], columns='unit_desc', values='Value').reset_index().set_index('week_ending')\n",
    "condition_state.rename(columns={'end_code': 'week_number'}, inplace=True)\n",
    "\n",
    "# Due to the API constrain, we get the data for 2014-2018 separately, and combine them together\n",
    "for year in range(2014, 2018):\n",
    "    condition_state_raw = dl.get_condition_data('WHEAT', 2014, exact_year=year, national_level=False, raw=True)\n",
    "\n",
    "    condition_state_raw['year'] = pd.to_numeric(condition_state_raw['year'])\n",
    "    # condition_national_raw['Value'] = condition_national_raw['Value'].str.replace(',', '', regex=True)\n",
    "    condition_state_raw['Value'] = pd.to_numeric(condition_state_raw['Value'], errors='coerce')\n",
    "    raw_data = condition_state_raw[condition_state_raw.class_desc == 'WINTER']\n",
    "\n",
    "    condition_state_year = raw_data.pivot(index=['week_ending', 'year', 'state_name', 'end_code'], columns='unit_desc', values='Value').reset_index().set_index('week_ending')\n",
    "    condition_state_year.rename(columns={'end_code': 'week_number'}, inplace=True)\n",
    "    condition_state = pd.concat([condition_state_year, condition_state], axis=0)\n",
    "condition_state.sort_index(inplace=True)\n",
    "condition_state"
   ],
   "id": "ef6294dadb1fff67"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Put all data together to a dataframe",
   "id": "367831f57b72432"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "data = market_data.copy().reset_index()\n",
    "for state in states_of_interest:\n",
    "    # appending the yearly production data of each state\n",
    "    yearly_production_data = production_state[\n",
    "        (production_state.state_name==state) &\n",
    "        (production_state.source_desc=='SURVEY')\n",
    "    ].pivot(index='year', columns='reference_period_desc', values='Value').reset_index().set_index('year').add_prefix(state+'_')\n",
    "    data = pd.merge(data, yearly_production_data, how='left', left_on='Year', right_on='year')\n",
    "\n",
    "    weekly_condition_data = condition_state[\n",
    "        condition_state.state_name==state\n",
    "    ][['PCT EXCELLENT', 'PCT FAIR', 'PCT GOOD', 'PCT POOR', 'PCT VERY POOR']].add_prefix(state+'_')\n",
    "    weekly_condition_data.index = pd.to_datetime(weekly_condition_data.index)\n",
    "\n",
    "    # appending the weekly condition data\n",
    "    data=pd.merge(data, weekly_condition_data, how='outer', left_on='Date', right_index=True)\n",
    "\n",
    "# appending the quarterly stocks data\n",
    "data = pd.merge(data, stocks_national, how='outer', left_on=['Year', 'Month'], right_on=['year', 'end_month'])\n",
    "\n",
    "# drop redundant columns, sort by dates, and perform ffill\n",
    "data = data.drop(['year', 'end_month', 'expiry'], axis=1)\n",
    "data.sort_values(by='Date', inplace=True)\n",
    "data = data.ffill(axis=0)\n",
    "\n",
    "# This will remove the extra date created when merging the tables\n",
    "data = data[data.Date.isin(market_data.index)]\n",
    "\n",
    "# Creating the target column, which is the 7day log return\n",
    "data['7d_log_return'] = np.log(data['Close']/data['Close'].shift(7))\n",
    "# data['Target'] = data['7d_log_return'].shift(-7)\n",
    "\n",
    "data.dropna(inplace=True)\n",
    "data.set_index('Date', inplace=True)\n",
    "data"
   ],
   "id": "9b0fd755838fd4b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "data.columns",
   "id": "96b9849593f79d5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "data['7d_log_return'].describe()",
   "id": "2198b341aabaea2b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import seaborn as sns\n",
    "sns.histplot(data=data, x='7d_log_return', kde='True')"
   ],
   "id": "e2620c57ff849a34"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import statsmodels.api as sm\n",
    "sm.graphics.tsa.plot_pacf(data['7d_log_return'], lags = 20)\n",
    "plt.show()"
   ],
   "id": "6469d608d69c8c1d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Building a LSTM model using the above data",
   "id": "f5d8a510adc3a0c3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "device = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ],
   "id": "7bbcf987fe7a682f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def create_sequences(data, seq_length):\n",
    "    sequences = []\n",
    "    targets = []\n",
    "    for i in range(len(data) - seq_length):\n",
    "        sequences.append(data[i:i+seq_length])\n",
    "        # the following code assumes that the target is the last column\n",
    "        targets.append(data[i+seq_length][-1])\n",
    "    return np.array(sequences), np.array(targets)\n",
    "\n",
    "data_train = data[:-500]\n",
    "data_test = data[-500:]\n",
    "\n",
    "# Now, this scaler will not cause data leakage\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "data_train= scaler.fit_transform(data_train)\n",
    "data_test = scaler.transform(data_test)\n",
    "data_scaled = np.vstack([data_train, data_test])\n",
    "\n",
    "SEQ_LENGTH = 60\n",
    "X, y = create_sequences(data_scaled, SEQ_LENGTH)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train, y_train = torch.tensor(X[:-500], dtype=torch.float32).to(device), torch.tensor(y[:-500], dtype=torch.float32).to(device)\n",
    "X_test, y_test = torch.tensor(X[-500:], dtype=torch.float32).to(device), torch.tensor(y[-500:], dtype=torch.float32).to(device)\n",
    "\n",
    "# Reshape for LSTM (batch_size, seq_length, num_features)\n",
    "X_train = X_train.view(-1, SEQ_LENGTH, 128)\n",
    "X_test = X_test.view(-1, SEQ_LENGTH, 128)\n",
    "\n",
    "# Create DataLoader\n",
    "BATCH_SIZE = 32\n",
    "train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(TensorDataset(X_test, y_test), batch_size=BATCH_SIZE, shuffle=False)\n"
   ],
   "id": "ea110e21c853612c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim=128, hidden_dim=64, num_layers=2, output_dim=1):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        out = self.fc(lstm_out[:, -1, :])  # Take last output from LSTM\n",
    "        return out\n",
    "\n",
    "# Initialize Model\n",
    "model = LSTMModel().to(device)"
   ],
   "id": "ac9029054f1fbae6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Define Loss and Optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training Loop\n",
    "EPOCHS = 1000\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for batch_x, batch_y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(batch_x)\n",
    "        loss = criterion(y_pred.squeeze(), batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{EPOCHS}], Loss: {epoch_loss/len(train_loader):.6f}')"
   ],
   "id": "c6d70a8310bd73b1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred_test = model(X_test).squeeze().cpu().numpy()\n",
    "\n",
    "y_pred_test_rescaled = y_pred_test.reshape(-1, 1)\n",
    "y_test_rescaled = y_test.cpu().numpy().reshape(-1, 1)\n",
    "\n",
    "# Plot results\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(data.index[-500:], y_test_rescaled, label='Actual')\n",
    "plt.plot(data.index[-500:], y_pred_test_rescaled, label='Predicted')\n",
    "plt.legend()\n",
    "plt.ylabel('7D log return')\n",
    "plt.title(\"LSTM Model Predictions on testing set (without inverse transform)\")\n",
    "plt.show()\n",
    "\n",
    "print(f\"r2 score {r2_score(y_test_rescaled, y_pred_test_rescaled):.4f}\")"
   ],
   "id": "94671faf96f1e43e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Transforming the data back to the usual scale\n",
    "res1 = scaler.inverse_transform(np.hstack([data_test[:, :-1], y_pred_test.reshape(-1, 1)]))\n",
    "res2 = scaler.inverse_transform(data_test)\n",
    "y_pred = res1[:, -1].reshape(-1, 1)\n",
    "y_test = res2[:, -1].reshape(-1, 1)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(data.index[-50:], y_test[-50:], label='Actual')\n",
    "plt.plot(data.index[-50:], y_pred[-50:], label='Predicted')\n",
    "plt.legend()\n",
    "plt.ylabel('7D log return')\n",
    "plt.title(\"LSTM Model Predictions on testing set (with inverse transform)\")\n",
    "plt.show()\n",
    "\n",
    "print(f\"r2 score {r2_score(y_test, y_pred):.4f}\")"
   ],
   "id": "9a67f13a0184b940"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "price_predicted = np.exp(y_pred) * (data.Close.shift(7)[-500:].to_numpy().reshape(-1, 1))\n",
    "actual_price = data.Close[-500:]\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.title(\"Predicted vs. Actual price\")\n",
    "plt.plot(data.index[-500:], actual_price, label='Actual')\n",
    "plt.plot(data.index[-500:], price_predicted, label='Predicted')\n",
    "plt.legend()\n",
    "print(f\"r2: {r2_score(actual_price, price_predicted)}\")\n",
    "\n",
    "def directional_accuracy(y_true, y_pred):\n",
    "    y_true, y_pred = np.asarray(y_true), np.asarray(y_pred)\n",
    "\n",
    "    # Compute price changes\n",
    "    actual_change = np.diff(y_true)  # y_t+1 - y_t\n",
    "    predicted_change = np.diff(y_pred)  # ŷ_t+1 - ŷ_t\n",
    "    correct_directions = np.sum((actual_change * predicted_change) > 0)\n",
    "    da = correct_directions / len(actual_change) * 100  # Convert to percentage\n",
    "\n",
    "    return da\n",
    "\n",
    "da = directional_accuracy(actual_price.to_numpy(), price_predicted.reshape(-1,))\n",
    "print(f\"Directional Accuracy: {da:.2f}%\")"
   ],
   "id": "d9301fe123e3ea8f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "double_check = pd.DataFrame({\n",
    "    \"predicted 7d log return\": y_pred.reshape(-1,),\n",
    "    \"7d log return from y test\": y_test.reshape(-1,),\n",
    "    \"from raw data\": data['7d_log_return'][-500:].to_numpy(),\n",
    "    \"Close_shifted\": data['Close'][-507:-7].to_numpy(),\n",
    "})\n",
    "\n",
    "double_check['predicted_price'] = np.exp(y_pred.reshape(-1,)) * double_check['Close_shifted']\n",
    "double_check['actual_price_calculated'] = np.exp(y_test.reshape(-1,)) * double_check['Close_shifted']\n",
    "double_check['Close'] = data['Close'][-500:].to_numpy()\n",
    "\n",
    "double_check"
   ],
   "id": "769f0f7558a5c57e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "actual_logreturn = np.log(actual_price[1:].to_numpy()/actual_price[:-1].to_numpy())\n",
    "# The wierd divide by 2 does the job, the log return r2 score is close now\n",
    "predicted_logreturn = np.log(price_predicted[1:]/price_predicted[:-1])\n",
    "plt.figure(figsize=(16, 6))\n",
    "plt.plot(actual_logreturn, label='Actual', alpha=0.5)\n",
    "plt.plot(predicted_logreturn, label='Predicted')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "print(f\"r2: {r2_score(actual_logreturn, predicted_logreturn)}\")"
   ],
   "id": "773887aa6fa32283"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "pd.DataFrame({\"predicted\": price_predicted.reshape(-1,), \"actual\": actual_price})",
   "id": "2aa5fe11a0478ced"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "logreturn_df = pd.DataFrame({\"predicted log return\": predicted_logreturn.reshape(-1,), \"actual log return\": actual_logreturn})\n",
    "print(f\"directional accuracy: {((logreturn_df['predicted log return'] * logreturn_df['actual log return'])>0).sum()/len(logreturn_df)}\")\n",
    "logreturn_df"
   ],
   "id": "54d8cfb5851b471c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "price_predicted_adjusted = np.exp(predicted_logreturn.reshape(-1,)) * data.Close[-500:-1]\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.title(\"Predicted_adjusted vs. Actual price\")\n",
    "plt.plot(data.index[-499:], actual_price[1:].to_numpy(), label='Actual')\n",
    "plt.plot(data.index[-499:], price_predicted_adjusted, label='Predicted')\n",
    "plt.legend()\n",
    "print(f\"r2: {r2_score(actual_price[1:], price_predicted_adjusted)}\")"
   ],
   "id": "571303d10cb9b2a8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df = pd.DataFrame({\"predicted\": price_predicted[1:].reshape(-1,), \"predicted_adjusted\": price_predicted_adjusted.to_numpy(), \"actual\": actual_price[1:].to_numpy()})\n",
    "price_predicted_adjusted_adjusted = (price_predicted_adjusted.to_numpy().reshape(-1,) + price_predicted[1:].reshape(-1,))/2\n",
    "df[\"adjusted_adjusted\"] = price_predicted_adjusted_adjusted\n",
    "df"
   ],
   "id": "1ae260e64e6ae02e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import TimeSeriesSplit, cross_val_score\n",
    "from sklearn.linear_model import LinearRegression"
   ],
   "id": "da93f5902fd81149"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "data['next_log_return'] = data['Log_Return'].shift(1)\n",
    "\n",
    "not_necessary_cols = ['Log_Return', '7d_log_return']\n",
    "\n",
    "data_train = data.drop(columns=not_necessary_cols).dropna()\n",
    "data_train\n",
    "\n",
    "X_train = data_train.drop(columns='next_log_return')[:-500]\n",
    "y_train = data_train['next_log_return'][:-500]\n",
    "\n",
    "X_test = data_train.drop(columns='next_log_return')[-500:]\n",
    "y_test = data_train['next_log_return'][-500:]\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(r2_score(y_test, y_pred))\n"
   ],
   "id": "8b2be9961585d671"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "y_pred",
   "id": "125fe2e87ef7e61d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Prediction for the next day price",
   "id": "89fd9ec80a62e2b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "TODO, think about the scaling. The wierd divide by 2 above. Use the price predicted_adjusted constructed above",
   "id": "36e97d2e1933ff92"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from datetime import datetime, timedelta\n",
    "def create_last_two_sequences(data, seq_length):\n",
    "    sequences = []\n",
    "\n",
    "    sequences.append(data[-seq_length-1:-1])\n",
    "    sequences.append(data[-seq_length:])\n",
    "    return np.array(sequences)\n",
    "\n",
    "data_test = scaler.transform(data)\n",
    "input = create_last_two_sequences(data_test, SEQ_LENGTH)\n",
    "\n",
    "input_tensor = torch.tensor(input, dtype=torch.float32).to(device)\n",
    "input_tensor = input_tensor.view(-1, SEQ_LENGTH, 128)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    output = model(input_tensor).squeeze().cpu().numpy()\n",
    "\n",
    "res1 = scaler.inverse_transform(np.concat([np.zeros(2*127).reshape(2, 127), output.reshape(2, 1)], axis=1))\n",
    "res1\n",
    "print(f\"The predicted next 7-day log return is {res1[:, -1][0]:.4f}\")\n",
    "print(f\"The predicted {(data.index[-1].date() + timedelta(days=1)).strftime(\"%Y-%m-%d\")} SRW Wheat Future Close Price is {(np.exp(res1[:, -1]) * data['Close'][-7])[0]:.4f}\")"
   ],
   "id": "b9f177c248bde792"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "res1[:,-1]",
   "id": "cd407a951ff0485c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "np.exp(res1[:, -1]) * data.Close[-8:-6]",
   "id": "1cfadd0e9c2b1f7c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "np.concat([np.zeros(2*127).reshape(2, 127), output.reshape(2, 1)], axis=1)",
   "id": "ae763ece2ff76e0c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "np.zeros(2*127).reshape(2, 127)",
   "id": "981fb01f33a94ca8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "output.reshape(2,1)",
   "id": "be2c9aa7592741b3"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
